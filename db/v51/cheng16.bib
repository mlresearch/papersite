@InProceedings{cheng16,
  supplementary = {Supplementary:cheng16-supp.pdf},
  title = {Scalable and Sound Low-Rank Tensor Learning},
  author = {Cheng, Hao and Yu, Yaoliang and Zhang, Xinhua and Xing, Eric and Schuurmans, Dale},
  pages = {1114-1123},
  abstract = {Many real-world data arise naturally as tensors. Equipped with a low rank prior, learning algorithms can benefit from exploiting the rich dependency encoded in a tensor. Despite its prevalence in low-rank matrix learning, trace norm ceases to be tractable in tensors and therefore most existing works resort to matrix unfolding. Although some theoretical guarantees are available, these approaches may lose valuable structure information and are not scalable in general. To address this problem, we propose directly optimizing the tensor trace norm by approximating its dual spectral norm, and we show that the approximation bounds can be efficiently converted to the original problem via the generalized conditional gradient algorithm. The resulting approach is scalable to large datasets, and matches state-of-the-art recovery guarantees. Experimental results on tensor completion and multitask learning confirm the superiority of the proposed method.},
}
