@InProceedings{abdullah16,
  title = {Sketching, Embedding and Dimensionality Reduction in Information Theoretic Spaces},
  author = {Abdullah, Amirali and Kumar, Ravi and McGregor, Andrew and Vassilvitskii, Sergei and Venkatasubramanian, Suresh},
  pages = {948-956},
  abstract = {In this paper we show how to embed information distances like the $\chi^2$ and Jensen-Shannon divergences efficiently in low dimensional spaces while preserving all  pairwise distances.  We then prove a dimensionality reduction result for the Hellinger, Jensen--Shannon, and $\chi^2$ divergences that preserves the information geometry of the distributions, specifically, by retaining the simplex structure of the space. While our first result already implies these divergences can be explicitly embedded in the Euclidean space, retaining the simplex structure is important because it allows us to do inferences in the reduced space.  We also show that these divergences  can be sketched efficiently (i.e., up to a multiplicative error in sublinear space) in the aggregate streaming model. This result is exponentially stronger than known upper bounds for sketching these distances in the strict turnstile streaming model.},
}
