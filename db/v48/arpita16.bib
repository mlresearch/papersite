@InProceedings{arpita16,
  supplementary = {Supplementary:arpita16-supp.pdf},
  title = {Why Regularized Auto-Encoders learn Sparse Representation?},
  author = {Devansh Arpit and Yingbo Zhou and Hung Ngo and Venu Govindaraju},
  pages = {136-144},
  abstract = {Sparse distributed representation is the key to learning useful features in deep learning algorithms, because not only it is an efficient mode of data representation, but also -- more importantly -- it captures the generation process of most real world data. While a number of regularized auto-encoders (AE) enforce sparsity explicitly in their learned representation and others don't, there has been little formal analysis on what encourages sparsity in these models in general. Our objective is to formally study this general problem for regularized auto-encoders. We provide sufficient conditions on both regularization and activation functions that encourage sparsity. We show that multiple popular models (de-noising and contractive auto encoders, e.g.) and activations (rectified linear and sigmoid, e.g.) satisfy these conditions; thus, our conditions help explain sparsity in their learned representation. Thus our theoretical and empirical analysis together shed light on the properties of regularization/activation that are conductive to sparsity and unify a number of existing auto-encoder models and activation functions under the same analytical framework.},
}
