@InProceedings{gong16,
  supplementary = {Supplementary:gong16-supp.pdf},
  title = {Domain Adaptation with Conditional Transferable Components},
  author = {Mingming Gong and Kun Zhang and Tongliang Liu and Dacheng Tao and Clark Glymour and Bernhard Sch√∂lkopf},
  pages = {2839-2848},
  abstract = {Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let $X$ and $Y$ denote the features and target, respectively, previous work on domain adaptation considers the covariate shift situation where the distribution of the features $P(X)$ changes across domains while the conditional distribution $P(Y|X)$ stays the same. To reduce domain discrepancy, recent methods try to find invariant components $\mathcal{T}(X)$ that have similar $P(\mathcal{T}(X))$ by explicitly minimizing a distribution discrepancy measure. However, it is not clear if $P(Y|\mathcal{T}(X))$ in different domains is also similar when $P(Y|X)$ changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identifiable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where $P(X|Y)$ and $P(Y)$ both change in a causal system in which $Y$ is the cause for $X$. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution $P(\mathcal{T}(X)|Y)$ is invariant after proper location-scale (LS) transformations, and identify how $P(Y)$ changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method.},
}
