@Proceedings{ICML-2013,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
month = {May},
year = {2013}}


@InProceedings{levine13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Guided Policy Search},
author = {Levine, Sergey and Koltun, Vladlen},
pages = {1-9},
abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.}
}

@InProceedings{niu13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning},
author = {Niu, Gang and Jitkrittum, Wittawat and Dai, Bo and Hachiya, Hirotaka and Sugiyama, Masashi},
pages = {10-18},
abstract = {We propose squared-loss mutual information regularization (SMIR) for multi-class probabilistic classification, following the information maximization principle. SMIR is convex under mild conditions and thus improves the nonconvexity of mutual information regularization. It offers all of the following four abilities to semi-supervised algorithms: Analytical solution, out-of-sample/multi-class classification, and probabilistic output. Furthermore, novel generalization error bounds are derived. Experiments show SMIR compares favorably with state-of-the-art methods.}
}

@InProceedings{szorenyi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Gossip-based distributed stochastic bandit algorithms},
author = {Szorenyi, Balazs and Busa-Fekete, Robert and Hegedus, Istvan and Ormandi, Robert and Jelasity, Mark and Kegl, Balazs},
pages = {19-27},
abstract = {The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms is available in each peer. In every iteration each peer can pull one arm independently of the other peers, and then some limited communication is possible with a few random other peers.  As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability of playing a suboptimal arm at a peer in iteration $t = \Omega( \log N )$ is proportional to $1/(Nt)$ where $N$ denotes the number of peers.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network.  }
}

@InProceedings{lattimore13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The Sample-Complexity of General Reinforcement Learning},
author = {Lattimore, Tor and Hutter, Marcus and Sunehag, Peter},
pages = {28-36},
abstract = {We study the sample-complexity of reinforcement learning in a general setting without  assuming ergodicity or finiteness of the environment. Instead, we define a topology  on the space of environments and show that  if an environment class is compact with respect to this topology then finite sample-complexity bounds are possible and give an  algorithm achieving these bounds. We also  show the existence of environment classes  that are non-compact where finite sample-complexity bounds are not achievable. A  lower bound is presented that matches the  upper bound except for logarithmic factors.  }
}

@InProceedings{zweig13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Hierarchical Regularization Cascade for Joint Learning},
author = {Zweig, Alon and Weinshall, Daphna},
pages = {37-45},
abstract = {As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased,until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods.}
}

@InProceedings{cortes13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Multi-Class Classification with Maximum Margin Multiple Kernel},
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
pages = {46-54},
abstract = {We present a new algorithm for multi-class classification with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel. We show that larger values of this quantity guarantee the existence of an accurate multi-class predictor and also define a family of multiple kernel algorithms based on the maximization of the multi-class margin of a kernel (M^3K).  We present an extensive theoretical analysis in support of our algorithm, including novel multi-class Rademacher complexity margin bounds.  Finally, we also report the results of a series of experiments with several data sets, including comparisons where we improve upon the performance of state-of-the-art algorithms both in binary and multi-class classification with multiple kernels.}
}

@InProceedings{grosshans13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Bayesian Games for Adversarial Regression Problems},
author = {Großhans, Michael and Sawade, Christoph and Brückner, Michael and Scheffer, Tobias},
pages = {55-63},
abstract = {We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary's objective; instead, any knowledge of the learner about parameters of the adversary's goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression.}
}

@InProceedings{chen13f,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing},
author = {Chen, Xi and Lin, Qihang and Zhou, Dengyong},
pages = {64-72},
abstract = {In real crowdsourcing applications, each label from a crowd usually comes  with a certain cost. Given a pre- fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to  find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However,  DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed.  We then extend  the MDP framework to deal with inhomogeneous workers and tasks with contextual information available.  The experiments on both simulated and real data  demonstrate the superiority of our method.}
}

@InProceedings{kolar13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Markov Network Estimation From Multi-attribute Data},
author = {Kolar, Mladen and Liu, Han and Xing, Eric},
pages = {73-81},
abstract = {Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating multi-attribute graphs. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features.  Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions.}
}

@InProceedings{zhang13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {MILEAGE: Multiple Instance LEArning with Global Embedding},
author = {Zhang, Dan and He, Jingrui and Si, Luo and Lawrence, Richard},
pages = {82-90},
abstract = {Multiple Instance Learning (MIL) methods generally represent each example as a collection of  instances such that the features for local objects can be better captured, whereas traditional learning methods typically extract a global feature vector for each example as an integral part. However, there is limited research work on which of the two learning scenarios performs better. This paper proposes a novel framework -- \emph{Multiple Instance LEArning with  Global Embedding (MILEAGE)}, in which  the global feature vectors for traditional learning methods are integrated into the MIL setting.  MILEAGE can leverage the benefits derived from both learning settings. Within the proposed framework, a large margin method is formulated. In particular,  the proposed method adaptively  tunes the weights on the two different kinds of feature  representations (i.e., global and multiple instance) for each example and trains the classifier simultaneously. An alternative algorithm is proposed to solve the resulting optimization problem, which extends the bundle method to the non-convex case. Some important properties of the proposed method, such as the convergence rate and the generalization error rate, are analyzed. A series of  experiments  have been conducted to demonstrate the advantages of the proposed method over several state-of-the-art multiple instance and traditional learning methods.}
}

@InProceedings{liu13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Guaranteed Sparse Recovery under Linear Transformation},
author = {Liu, Ji and Yuan, Lei and Ye, Jieping},
pages = {91-99},
abstract = {We consider the following signal recovery problem: given a  measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy  observation vector $c\in \mathbb{R}^{n}$ constructed from $c =  \Phi\theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the  noise vector whose entries follow i.i.d. centered sub-Gaussian  distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is  sparse {\rca under a linear transformation} $D\in\mathbb{R}^{m\times  p}$? One natural method using convex optimization is to solve the  following problem: $$\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 +  \lambda\|D\theta\|_1.$$ This paper provides an upper bound of the  estimate error and shows the consistency property of this method by  assuming that the design matrix $\Phi$ is a Gaussian random matrix.  Specifically, we show 1) in the noiseless case, if the condition  number of $D$ is bounded and the measurement number $n\geq  \Omega(s\log(p))$ where $s$ is the sparsity number, then the true  solution can be recovered with high probability; and 2) in the noisy  case, if the condition number of $D$ is bounded and the measurement  increases faster than $s\log(p)$, that is, $s\log(p)=o(n)$, the  estimate error converges to zero with probability 1 when $p$ and $s$  go to infinity. Our results are consistent with those for the  special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and  improve the existing analysis. The condition number of $D$ plays a  critical role in our analysis. We consider the condition numbers in  two cases including the fused LASSO and the random graph: the  condition number in the fused LASSO case is bounded by a constant,  while the condition number in the random graph case is bounded with  high probability if $m\over p$ (i.e., $\#\text{edge}\over  \#\text{vertex}$) is larger than a certain constant. Numerical  simulations are consistent with our theoretical results.}
}

@InProceedings{memisevic13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning invariant features by harnessing the aperture problem},
author = {Memisevic, Roland and Exarchakis, Georgios},
pages = {100-108},
abstract = {The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset.  }
}

@InProceedings{wauthier13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Efficient Ranking from Pairwise Comparisons},
author = {Wauthier, Fabian and Jordan, Michael and Jojic, Nebojsa},
pages = {109-117},
abstract = {The ranking of $n$ objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all $n(n-1)/2$ comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an $\Omega(n)$ lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same $\Omega(n)$ lower bound in expectation. Furthermore, if an average of $O(n\log(n))$ binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives.  }
}

@InProceedings{jain13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Differentially Private Learning with Kernels},
author = {Jain, Prateek and Thakurta, Abhradeep},
pages = {118-126},
abstract = {In this paper, we consider the problem of differentially private learning where access to the training features is through a kernel function only. Existing work on this problem is restricted to translation invariant kernels only, where (approximate) training features are available explicitly.  In fact, for general class of kernel functions and in general setting of releasing different private predictor ($\w^*$), the problem is impossible to solve \cite{CMS11}. In this work, we relax the problem setting into three different easier but practical settings. In our first problem setting, we consider an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. This setting is prevalent in modern online systems like search engines, ad engines etc. In the second model, the learner sends back a differentially private version of the optimal parameter vector $\w^*$ but requires access to a small subset of unlabeled test set beforehand. This also is a practical setting that involves two parties interacting through trusted third party. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of $\w^*$, but the kernels are restricted to efficiently computable functions over low-dimensional vector spaces. For each of the models, we derive differentially private learning algorithms with provable ``utlity'' or error bounds. Moreover, we  show that our methods can also be applied to the traditional setting of \cite{ Rubinstein09, CMS11}. Here, our sample complexity bounds have only $O(d^{1/3})$ dependence on the dimensionality $d$ while existing methods require $O(d^{1/2})$ samples to achieve same generalization error.}
}

@InProceedings{agrawal13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Thompson Sampling for Contextual Bandits with Linear Payoffs},
author = {Agrawal, Shipra and Goyal, Navin},
pages = {127-135},
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of $\tilde{O}(\frac{d}{\sqrt{\epsilon}}\sqrt{T^{1+\epsilon}})$ in time $T$ for any $\epsilon \in (0,1)$, where $d$ is the dimension of each context vector and $\epsilon$ is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of $\Omega(\sqrt{dT})$ for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem with linear payoff functions.     Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of other distributions, satisfying certain general conditions.   }
}

@InProceedings{almingol13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Controller Space},
author = {Almingol, Javier and Montesano, Lui and Lopes, Manuel},
pages = {136-144},
abstract = {In this paper we introduce a method to learn multiple behaviors in the form of motor primitives from an unlabeled dataset. One of the difficulties of this problem is that in the measurement space, behaviors can be very mixed, despite existing a latent representation where they can be easily separated.  We propose a mixture model based on Dirichlet Process (DP) to simultaneously cluster the observed time-series and recover a sparse representation of the behaviors using a Laplacian prior as the base measure of the DP. We show that for linear models, e.g potential functions generated by linear combinations of a large number of features, it is possible to compute analytically the marginal of the observations and derive an efficient sampler. The method is evaluated using robot behaviors and real data from human motion and compared to other techniques.  }
}

@InProceedings{kolmogorov13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Inference algorithms for pattern-based CRFs on sequence data},
author = {Kolmogorov, Vladimir and Takhanov, Rustem},
pages = {145-153},
abstract = {We consider {\em Conditional Random Fields (CRFs) with pattern-based potentials}  defined on a chain. In this model the energy of a string (labeling) $x_1\ldots x_n$  is the sum of terms over intervals $[i,j]$ where each term is non-zero only if the substring $x_i\ldots x_j$  equals a prespecified pattern $\alpha$. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely  computing (i) the partition function, (ii) marginals, and (iii) computing the MAP.  Their complexities are respectively  $O(n L)$, $O(n L \ell_{\max})$ and  $O(n L \min\{|D|,\log (\ell_{\max} + 1)\})$  where $L$ is the combined length of input patterns, $\ell_{\max}$ is the maximum length of a pattern,  and $D$ is the input alphabet.  This improves on the previous algorithms of~\cite{Ye:NIPS09} whose complexities are respectively $O(n L |D|)$,  $O\left(n |\Gamma| L^2 \ell_{\max}^2\right)$ and $O(n L |D|)$,  where $|\Gamma|$ is the number of input patterns. In addition, we give an efficient algorithm for sampling,  and revisit the case of MAP with non-positive weights. Finally, we apply pattern-based CRFs to the problem of the protein dihedral angles prediction.  }
}

@InProceedings{gopi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {One-Bit Compressed Sensing: Provable Support and Vector Recovery},
author = {Gopi, Sivakant and Netrapalli, Praneeth and Jain, Prateek and Nori, Aditya},
pages = {154-162},
abstract = {In this paper, we study the problem of one-bit compressed sensing ($1$-bit CS), where the goal is to design a measurement matrix $A$ and a recovery algorithm s.t. a $k$-sparse vector $\x^*$ can be efficiently recovered back from signed linear measurements, i.e., $b=\sign(A\x^*)$. This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \cite{HsuKLZ10}. We study this problem in two settings: a) support recovery: recover $\supp(\x^*)$, b) approximate vector recovery: recover a unit vector $\hx$ s.t. $|| \hat{x}-\x^*/||\x^*|| ||_2\leq \epsilon$. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix $A$ can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods. }
}

@InProceedings{tang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Tensor Analyzers},
author = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
pages = {163-171},
abstract = {Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.}
}

@InProceedings{hocking13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression},
author = {Hocking, Toby and Rigaill, Guillem and Vert, Jean-Philippe and Bach, Francis},
pages = {172-180},
abstract = {  In segmentation models, the number of change-points is typically    chosen using a penalized cost function.  In this work, we propose to    learn the penalty and its constants in databases of signals with    weak change-point annotations. We propose a convex relaxation for    the resulting interval regression problem, and solve it using    accelerated proximal gradient methods. We show that this method    achieves state-of-the-art change-point detection in a database of    annotated DNA copy number profiles from neuroblastoma tumors.}
}

@InProceedings{jun13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning from Human-Generated Lists},
author = {Jun, Kwang-Sung and Zhu, Jerry and Settles, Burr and Rogers, Timothy},
pages = {181-189},
abstract = {Human-generated lists are a form of non-iid data with important applications in machine learning and cognitive psychology. We propose a generative model - sampling with reduced replacement (SWIRL) - for such lists. We discuss SWIRL's relation to standard sampling paradigms, provide the maximum likelihood estimate for learning, and demonstrate its value with two real-world applications: (i) In a ""feature volunteering"" task where non-experts spontaneously generate feature=>label pairs for text classification, SWIRL improves the accuracy of state-of-the-art feature-learning frameworks. (ii) In a ""verbal fluency"" task where brain-damaged patients generate word lists when prompted with a category, SWIRL parameters align well with existing psychological theories, and our model can classify healthy people vs. patients from the lists they generate.    }
}

@InProceedings{wang13f,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {A Fast and Exact Energy Minimization Algorithm for Cycle MRFs},
author = {Wang, Huayan and Daphne, Koller},
pages = {190-198},
abstract = {The presence of cycles gives rise to the difficulty in performing inference for MRFs. Handling cycles efficiently would greatly enhance our ability to tackle general MRFs. In particular, for dual decomposition of energy minimization (MAP inference), using cycle subproblems leads   to a much tighter relaxation than using trees, but solving the cycle subproblems turns out to be the bottleneck.  In this paper, we present a fast and exact algorithm for energy minimization in cycle MRFs, which can be used as a subroutine in tackling general MRFs. Our method builds on junction-tree message passing, with a large portion of the message entries pruned for efficiency. The pruning conditions fully exploit the structure of a cycle. Experimental results show that our algorithm is more than an order of magnitude faster than other state-of-the-art fast inference methods, and it performs consistently well in several different real problems.}
}

@InProceedings{tarlow13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning},
author = {Tarlow, Daniel and Swersky, Kevin and Charlin, Laurent and Sutskever, Ilya and Zemel, Rich},
pages = {199-207},
abstract = {Neighborhood Components Analysis (NCA) is a popular method for  learning a distance metric to be used within a k-nearest neighbors  (kNN) classifier.    A key assumption built into the model is that each point  stochastically selects a single neighbor, which  makes the model well-justified only for kNN with k=1.  However, kNN classifiers with k>1 are more robust and usually   preferred in practice.     Here we present kNCA, which generalizes NCA by  learning distance metrics that are appropriate for  kNN with arbitrary k.  The main technical contribution is showing  how to efficiently compute and optimize the expected  accuracy of a kNN classifier.  We apply similar ideas in an unsupervised  setting to yield kSNE and ktSNE, generalizations of  Stochastic Neighbor Embedding (SNE, tSNE) that operate on  neighborhoods of size k, which provide an axis of control over  embeddings that allow for more homogeneous and interpretable regions.  Empirically, we show that kNCA often improves classification accuracy over  state of the art methods, produces qualitative  differences in the embeddings as k is varied, and is more robust with  respect to label noise. }
}

@InProceedings{bryan13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {An Efficient Posterior Regularized Latent Variable Model for Interactive Sound Source Separation},
author = {Bryan, Nicholas and Mysore, Gautham},
pages = {208-216},
abstract = {In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single-channel recording into its respective sources.  One of the current most effective class of methods to do so is based on non-negative matrix factorization and related latent variable models.  Such techniques, however, typically perform poorly when no isolated training data is given and do not allow user feedback to correct for poor results. To overcome these issues, we allow a user to interactively constrain a latent variable model by painting on a time-frequency display of sound to guide the learning process.  The annotations are used within the framework of posterior regularization to impose linear grouping constraints that would otherwise be difficult to achieve via standard priors.  For the constraints considered, an efficient expectation-maximization algorithm is derived with closed-form multiplicative updates, drawing connections to non-negative matrix factorization methods, and allowing for high-quality interactive-rate separation without explicit training data.}
}

@InProceedings{lopes13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Estimating Unknown Sparsity in Compressed Sensing},
author = {Lopes, Miles},
pages = {217-225},
abstract = {In the theory of compressed sensing (CS), the sparsity $\|x\|_0$ of the unknown signal $x\in\R^p$ is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of  CS depend on knowing $\|x\|_0$, it is important to estimate this parameter in a data-driven way. A second practical concern is that $\|x\|_0$ is a highly unstable function of $x$. In particular, for real signals with entries not exactly equal to 0, the value $\|x\|_0=p$ is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity $s(x):=\|x\|_1^2/\|x\|_2^2$, which is a sharp lower bound on $\|x\|_0$. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for $s(x)$ is  provided, and its width is shown to have no dependence on the signal dimension $p$. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating $s(x)$. This is accomplished by proving that the minimax risk for estimating $s(x)$ with deterministic measurements is large when $n\ll p$.}
}

@InProceedings{broderick13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {MAD-Bayes: MAP-based Asymptotic Derivations from Bayes},
author = {Broderick, Tamara and Kulis, Brian and Jordan, Michael},
pages = {226-234},
abstract = {The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.}
}

@InProceedings{peharz13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The Most Generative Maximum Margin Bayesian Networks},
author = {Peharz, Robert and Tschiatschek, Sebastian and Pernkopf, Franz},
pages = {235-243},
abstract = {Although discriminative learning in graphical models generally improves classification results, the generative semantics of the model are compromised.  In this paper, we introduce a novel approach of hybrid generative-discriminative learning for Bayesian networks.  We use an SVM-type large margin formulation for discriminative training, introducing a likelihood-weighted $\ell^1$-norm for the SVM-norm-penalization.  This simultaneously optimizes the data likelihood and therefore partly maintains the generative character of the model.  For many network structures, our method can be formulated as a convex problem, guaranteeing a globally optimal solution.  In terms of classification, the resulting models outperform state-of-the art generative and discriminative learning methods for Bayesian networks, and are comparable with linear and kernelized SVMs.  Furthermore, the models achieve likelihoods close to the maximum likelihood solution and show robust behavior in classification experiments with missing features.}
}

@InProceedings{le13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fastfood - Computing Hilbert Space Expansions in loglinear time},
author = {Le, Quoc and Sarlos, Tamas and Smola, Alexander},
pages = {244-252},
abstract = {Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.  }
}

@InProceedings{chattopadhyay13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Joint Transfer and Batch-mode Active Learning},
author = {Chattopadhyay, Rita and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},
pages = {253-261},
abstract = {Active learning and transfer learning are two different methodologies that address the common  problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples  for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning  methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a  single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5\% to  10\% over the existing two-stage approach}
}

@InProceedings{qi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Message passing with l1 penalized KL minimization},
author = {Qi, Yuan and Guo, Yandong},
pages = {262-270},
abstract = {Bayesian inference is often hampered by large computational expense.  As a generalization of belief propagation (BP), expectation propagation (EP) approximates exact Bayesian computation with efficient message passing updates. However, when an approximation family used by EP is far from exact posterior distributions, message passing may lead to poor approximation quality and suffer from divergence. To address this issue, we propose an  approximate inference method, relaxed expectation propagation(REP), based on a new divergence with a l1 penalty. Minimizing this penalized divergence adaptively relaxes EP's moment matching requirement for message passing. We apply REP to Gaussian process classification and experimental results demonstrate significant improvement of REP over EP and alpha-divergence based power EP -- in terms of algorithmic stability, estimation accuracy, and predictive performance. Furthermore, we develop relaxed belief propagation(RBP), a special case of REP, to conduct inference on discrete Markov random fields (MRFs). Our results show improved estimation accuracy of RBP over BP and fractional BP when interactions between MRF nodes are strong.}
}

@InProceedings{cuturi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Mean Reversion with a Variance Threshold},
author = {Cuturi, Marco and D'Aspremont, Alexandre},
pages = {271-279},
abstract = {Starting from a multivariate data set, we study several techniques to isolate affine combinations of the variables with a maximum amount of mean reversion, while constraining the variance to be larger than a given threshold. We show that many of the optimization problems arising in this context can be solved exactly using semidefinite programming and some variant of the $\mathcal{S}$-lemma. In finance, these methods are used to isolate statistical arbitrage opportunities, i.e. mean reverting portfolios with enough variance to overcome market friction. In a more general setting, mean reversion and its generalizations are also used as a proxy for stationarity, while variance simply measures signal strength.}
}

@InProceedings{lakshminarayanan13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Top-down particle filtering for Bayesian decision trees},
author = {Lakshminarayanan, Balaji and Roy, Daniel and Whye Teh, Yee},
pages = {280-288},
abstract = {Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulationswhich introduce a prior distribution over decision trees, and formulate learning as posterior inference given datahave been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff.}
}

@InProceedings{balasubramanian13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations},
author = {Balasubramanian, Krishnakumar and Yu, Kai and Lebanon, Guy},
pages = {289-297},
abstract = {We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning  using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the  speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semisupervised sparse coding.}
}

@InProceedings{wang13g,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Robust and Discriminative Self-Taught Learning},
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
pages = {298-306},
abstract = {The lack of training data is a common challenge in many machine learning problems, which is often tackled by semi-supervised learning methods or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data. We derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.}
}

@InProceedings{pirotta13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Safe Policy Iteration},
author = {Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
pages = {307-315},
abstract = {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.}
}

@InProceedings{ishteva13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Unfolding Latent Tree Structures using 4th Order Tensors},
author = {Ishteva, Mariya and Park, Haesun and Song, Le},
pages = {316-324},
abstract = {Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.}
}

@InProceedings{zemel13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Fair Representations},
author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
pages = {325-333},
abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}

@InProceedings{song13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Hierarchical Tensor Decomposition of Latent Tree Graphical Models},
author = {Song, Le and Ishteva, Mariya and Parikh, Ankur and Xing, Eric and Park, Haesun},
pages = {334-342},
abstract = {We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. In this new view, the marginal probability table of the observed variables in a latent tree is treated as a tensor, and we show that: (i) the latent variables induce low rank structures in various matricizations of the tensor; (ii) this collection of low rank matricizations induce a hierarchical low rank decomposition of the tensor. Exploiting these properties, we derive an optimization problem for estimating the parameters of a latent tree graphical model, i.e., hierarchical decomposion of a tensor which minimizes the Frobenius norm of the difference between the original tensor and its decomposition.    When the latent tree graphical models are correctly specified, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm. This algorithm recovers previous spectral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) as special cases, elucidating the global objective these algorithms are optimizing for. When the latent tree graphical models are misspecified, we derive a better decomposition based on our framework, and provide approximation guarantee for this new estimator. In both synthetic and real world data, this new estimator significantly improves over the-state-of-the-art.}
}

@InProceedings{schaul13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {No more pesky learning rates},
author = {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
pages = {343-351},
abstract = {The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.}
}

@InProceedings{wang13c,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Multi-View Clustering and Feature Learning via Structured Sparsity},
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
pages = {352-360},
abstract = {Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.}
}

@InProceedings{vanseijen13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Planning by Prioritized Sweeping with Small Backups},
author = {Van Seijen, Harm and Sutton, Rich},
pages = {361-369},
abstract = {Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that  uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations. }
}

@InProceedings{brechtel13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation},
author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, Rdiger},
pages = {370-378},
abstract = {Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems.}
}

@InProceedings{dalalyan13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Heteroscedastic Models by Convex Programming under Group Sparsity},
author = {Dalalyan, Arnak and Hebiri, Mohamed and Meziani, Katia and Salmon, Joseph},
pages = {379-387},
abstract = {Sparse estimation methods based on l1 relaxation,  such as the Lasso and the Dantzig  selector, require the knowledge of the variance  of the noise in order to properly tune the  regularization parameter. This constitutes a   major obstacle in applying these methods in  several frameworks, such as time series, random fields, inverse problems, for which noise  is rarely homoscedastic and the noise level is  hard to know in advance.  In this paper, we  propose a new approach to the joint estimation  of the conditional mean and the conditional  variance in a high-dimensional (auto-)  regression setting.   An attractive feature of the proposed estimator is   that it is efficiently computable even for very large   scale problems by solving a second-order cone program  (SOCP). We present theoretical analysis and  numerical results assessing the performance  of the proposed procedure.}
}

@InProceedings{zhang13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels},
author = {Zhang, Kai and Zheng, Vincent and Wang, Qiaojun and Kwok, James and Yang, Qiang and Marsic, Ivan},
pages = {388-395},
abstract = {Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercers theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications.}
}

@InProceedings{allenzhu13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {A Local Algorithm for Finding Well-Connected Clusters},
author = {Allen Zhu, Zeyuan and Lattanzi, Silvio and Mirrokni, Vahab},
pages = {396-404},
abstract = {Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data.    More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering.  }
}

@InProceedings{bi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Efficient Multi-label Classification with Many Labels},
author = {Bi, Wei and Kwok, James},
pages = {405-413},
abstract = {Multi-label classification deals with the problem where each instance can be associated with a set of class labels. However, in many real-world applications, the number of class labels can be in the hundreds or even thousands, and existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are either based on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by randomized sampling where the sampling probability of each class label reflects its importance among all the labels. Theoretical analysis shows that this randomized sampling approach is highly efficient. Experiments on a number of real-world multi-label datasets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm.  }
}

@InProceedings{chen13g,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Spectral Compressed Sensing via Structured Matrix Completion},
author = {Chen, Yuxin and Chi, Yuejie},
pages = {414-422},
abstract = {The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension $n$ is assumed to be a mixture of $r$ complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the {\em basis mismatch} issue when imposing a discrete dictionary on the Fourier representation. To address this problem,  we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by converting the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of $\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments.  }
}

@InProceedings{yang13d,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Multi-Task Learning with Gaussian Matrix Generalized Inverse Gaussian Model},
author = {Yang, Ming and Li, Yingming and Zhang (Zhejiang University), Zhongfei},
pages = {423-431},
abstract = {In this paper, we study the multi-task learning problem with a new perspective of considering the structure of the residue error matrix and the low-rank approximation to the task covariance matrix simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) prior and define a Gaussian Matrix Generalized Inverse Gaussian (GMGIG) model for low-rank approximation to the task covariance matrix. Through combining the GMGIG model with the residual error structure assumption, we propose the GMGIG regression model for multi-task learning. To make the computation tractable, we simultaneously use variational inference and sampling techniques. In particular, we propose two sampling strategies for computing the statistics of the MGIG distribution. Experiments show that this model is superior to the peer methods in regression and prediction.}
}

@InProceedings{cho13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images},
author = {Cho, Kyunghyun},
pages = {432-440},
abstract = {Recently Burger et al. (2012) and Xie et al. (2012) proposed to use a denoising autoencoder (DAE) for denoising noisy images. They showed that a plain, deep DAE can denoise noisy images as well as the conventional methods such as BM3D and KSVD. Both of them approached image denoising by denoising small, image patches of a larger image and combining them to form a clean image. In this setting, it is usual to use the encoder of the DAE to obtain the latent representation and subsequently apply the decoder to get the clean patch. We propose that a simple sparsification of the latent representation found by the encoder improves denoising performance, when the DAE was trained with sparsity regularization. The experiments confirm that the proposed sparsification indeed helps both denoising a small image patch and denoising a larger image consisting of those patches. Furthermore, it is found out that the proposed method improves even classification performance when test samples are corrupted with noise.}
}

@InProceedings{kar13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions},
author = {Kar, Purushottam and Sriperumbudur, Bharath and Jain, Prateek and Karnick, Harish},
pages = {441-449},
abstract = {In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.}
}

@InProceedings{baktashmotlagh13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Non-Linear Stationary Subspace Analysis with Application to Video Classification},
author = {Baktashmotlagh, Mahsa and Harandi, Mehrtash and Bigdeli, Abbas and Lovell, Brian and Salzmann, Mathieu},
pages = {450-458},
abstract = {Low-dimensional representations are key to the success of many video classification algorithms. However, the commonly-used dimensionality reduction techniques fail to account for the fact that only part of the signal is shared across all the videos in one class. As a consequence, the resulting representations contain instance-specific information, which introduces noise in the classification process. In this paper, we introduce Non-Linear Stationary Subspace Analysis: A method that overcomes this issue by explicitly separating the stationary parts of the video signal (i.e., the parts shared across all videos in one class), from its non-stationary parts (i.e., specific to individual videos). We demonstrate the effectiveness of our approach on action recognition, dynamic texture classification and scene recognition.}
}

@InProceedings{honorio13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy},
author = {Honorio, Jean and Tommi, Jaakkola},
pages = {459-467},
abstract = {We provide a method that approximates the Bayes error rate and the Shannon entropy with high probability. The Bayes error rate approximation makes possible to build a classifier that polynomially approaches Bayes error rate. The Shannon entropy approximation provides provable performance guarantees for learning trees and Bayesian networks from continuous variables. Our results rely on some reasonable regularity conditions of the unknown probability distributions, and apply to bounded as well as unbounded variables.  }
}

@InProceedings{coviello13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {That was fast! Speeding up NN search of high dimensional distributions.},
author = {Coviello, Emanuele and Mumtaz, Adeel and Chan, Antoni and Lanckriet, Gert},
pages = {468-476},
abstract = {We present a data structure for fast nearest neighbor retrieval of generative models of documents based on KL divergence.  Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database,   and uses a novel branch and bound methodology for search.  The main technical contribution of the paper is a   novel and efficient algorithm  for deciding whether to explore nodes during backtracking, based on a variational approximation.  This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data.  In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions.    Experiments demonstrate substantial speed-ups over both Bregman  Ball Trees and over brute force search, on both moderate and high dimensional histogram data.  In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models.}
}

@InProceedings{vladymyrov13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Entropic Affinities: Properties and Efficient Numerical Computation},
author = {Vladymyrov, Max and Carreira-Perpinan, Miguel},
pages = {477-485},
abstract = {Gaussian affinities are commonly used in graph-based methods such as spectral clustering or nonlinear embedding. Hinton and Roweis (2003) introduced a way to set the scale individually for each point so that it has a distribution over neighbors with a desired perplexity, or effective number of neighbors. This gives very good affinities that adapt locally to the data but are harder to compute. We study the mathematical properties of these ``entropic affinities'' and show that they implicitly define a continuously differentiable function in the input space and give bounds for it. We then devise a fast algorithm to compute the widths and affinities, based on robustified, quickly convergent root-finding methods combined with a tree- or density-based initialization scheme that exploits the slowly-varying behavior of this function. This algorithm is nearly optimal and much more accurate and fast than the existing bisection-based approach, particularly with large datasets, as we show with image and text data.}
}

@InProceedings{jose13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Local Deep Kernel Learning for Efficient Non-linear SVM Prediction},
author = {Jose, Cijo and Goyal, Prasoon and Aggrwal, Parv and Varma, Manik},
pages = {486-494},
abstract = {Our objective is to speed up non-linear SVM prediction while maintaining classification accuracy above an acceptable limit. We generalize Localized Multiple Kernel Learning so as to learn a primal feature space embedding which is high dimensional, sparse and computationally deep. Primal based classification decouples prediction costs from the number of support vectors and our tree-structured features efficiently encode non-linearities while speeding up prediction exponentially over the state-of-the-art. We develop routines for optimizing over the space of tree-structured features and efficiently scale to problems with over half a million training points. Experiments on benchmark data sets reveal that our formulation can reduce prediction costs by more than three orders of magnitude in some cases with a moderate sacrifice in classification accuracy as compared to RBF-SVMs. Furthermore, our formulation leads to much better classification accuracies over leading methods.}
}

@InProceedings{tamar13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Temporal Difference Methods for the Variance of the Reward To Go},
author = {Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},
pages = {495-503},
abstract = {In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose variants of both TD(0) and LSTD($\lambda$) with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem.}
}

@InProceedings{yu13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {$\propto$SVM for Learning with Label Proportions},
author = {Yu, Felix and Liu, Dong and Kumar, Sanjiv and Tony, Jebara and Chang, Shih-Fu},
pages = {504-512},
abstract = {We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or $\propto$SVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The $\propto$SVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that $\propto$SVM outperforms the state-of-the-art, especially for larger group sizes.}
}

@InProceedings{kraehenbuehl13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Parameter Learning and Convergent Inference for Dense Random Fields},
author = {Kraehenbuehl, Philipp and Koltun, Vladlen},
pages = {513-521},
abstract = {Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model.  As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge.}
}

@InProceedings{mineiro13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Loss-Proportional Subsampling for Subsequent ERM},
author = {Mineiro, Paul and Karampatziakis, Nikos},
pages = {522-530},
abstract = {We propose a sampling scheme suitable for reducing a data set prior to  selecting a hypothesis with minimum empirical risk.  The sampling only  considers a subset of the ultimate (unknown) hypothesis set, but can  nonetheless guarantee that the final excess risk will compare favorably  with utilizing the entire original data set. We demonstrate the practical  benefits of our approach on a large dataset which we subsample and  subsequently fit with boosted trees.}
}

@InProceedings{meng13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Scalable Simple Random Sampling and Stratified Sampling},
author = {Meng, Xiangrui},
pages = {531-539},
abstract = {  Analyzing data sets of billions of records has now become a regular task in    many companies and institutions.    In the statistical analysis of those massive data sets, sampling generally    plays a very important role.    In this work, we describe a scalable simple random sampling algorithm, named    ScaSRS, which uses probabilistic thresholds to decide on the fly whether to    accept, reject, or wait-list an item independently of others.    We prove, with high probability, it succeeds and needs only $O(\sqrt{k})$    storage, where $k$ is the sample size.    ScaSRS extends naturally to a scalable stratified sampling algorithm, which is    favorable for heterogeneous data sets.    The proposed algorithms, when implemented in MapReduce, can effectively reduce    the size of intermediate output and greatly improve load balancing.    Empirical evaluation on large-scale data sets clearly demonstrates their    superiority.}
}

@InProceedings{cheng13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Riemannian Similarity Learning},
author = {Cheng, Li},
pages = {540-548},
abstract = {We consider a similarity-score based paradigm to address scenarios where either the class labels are only partially revealed during learning, or the training and testing data are drawn from heterogeneous sources. The learning problem is subsequently formulated as optimization over a bilinear form of fixed rank. Our paradigm bears similarity to metric learning, where the major difference lies in its aim of learning a rectangular similarity matrix, instead of a proper metric. We tackle this problem in a Riemannian optimization framework. In particular, we consider its applications in pairwise-based action recognition, and cross-domain image-based object recognition. In both applications, the proposed algorithm produces competitive performance on respective benchmark datasets.}
}

@InProceedings{jia13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On Compact Codes for Spatially Pooled Features},
author = {Jia, Yangqing and Vinyals, Oriol and Darrell, Trevor},
pages = {549-557},
abstract = {Feature encoding with an overcomplete dictionary has demonstrated good performance in many applications, especially computer vision. In this paper we analyze the classification accuracy with respect to dictionary size by linking the encoding stage to kernel methods and \nystrom sampling, and obtain useful bounds on accuracy as a function of size. The \nystrom method also inspires us to revisit dictionary learning from local patches, and we propose to learn the dictionary in an end-to-end fashion taking into account pooling, a common computational layer in vision. We validate our contribution by showing how the derived bounds are able to explain the observed behavior of multiple datasets, and show that the pooling aware method efficiently reduces the dictionary size by a factor of two for a given accuracy.}
}

@InProceedings{wu13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Dynamic Covariance Models for Multivariate Financial Time Series},
author = {Wu, Yue and Miguel Hernandez-Lobato, Jose and Zoubin, Ghahramani},
pages = {558-566},
abstract = {The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.}
}

@InProceedings{gittens13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Revisiting the Nystrom method for improved large-scale machine learning},
author = {Gittens, Alex and Mahoney, Michael},
pages = {567-575},
abstract = {We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications.    Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores.    We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds--- e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.}
}

@InProceedings{yoshii13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals},
author = {Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},
pages = {576-584},
abstract = {This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.}
}

@InProceedings{yang13e,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {A Unified Robust Regression Model for Lasso-like Algorithms},
author = {Yang, Wenzhuo and Xu, Huan},
pages = {585-593},
abstract = {We develop a unified robust linear regression model and show that it is equivalent to a general regularization framework to encourage sparse-like structure that contains group Lasso and fused Lasso as specific examples. This provides a robustness interpretation of these widely applied Lasso-like algorithms, and allows us to construct novel generalizations of Lasso-like algorithms by considering different uncertainty sets. Using this robustness interpretation, we present new sparsity results, and establish the statistical consistency of the proposed regularized linear regression. This work extends a classical result from Xu et al. (2010) that relates standard Lasso with robust linear regression to learning problems with more general sparse-like structures, and provides new robustness-based tools to to understand learning problems with sparse-like structures.}
}

@InProceedings{appel13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Quickly Boosting Decision Trees -- Pruning Underachieving Features Early},
author = {Appel, Ron and Fuchs, Thomas and Dollar, Piotr and Perona, Pietro},
pages = {594-602},
abstract = {Boosted decision trees are one of the most popular and successful learning techniques used today.  While exhibiting fast speeds at test time, relatively slow training makes them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early on in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it may be used in conjunction with existing Boosting algorithms and other sampling heuristics to achieve even greater speedups.}
}

@InProceedings{menon13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance},
author = {Menon, Aditya and Narasimhan, Harikrishna and Agarwal, Shivani and Chawla, Sanjay},
pages = {603-611},
abstract = {Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems.  }
}

@InProceedings{chuang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment},
author = {Chuang, Jason and Gupta, Sonal and Manning, Christopher and Heer, Jeffrey},
pages = {612-620},
abstract = {The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure they are meaningful. We introduce a framework to support large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality.}
}

@InProceedings{zhang13c,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Online Kernel Learning with a Near Optimal Sparsity Bound},
author = {Zhang, Lijun and Yi, Jinfeng and Jin, Rong and Lin, Ming and He, Xiaofei},
pages = {621-629},
abstract = {In this work, we focus on Online Sparse Kernel Learning that aims to online learn a kernel classifier with a bounded number of support vectors. Although many online learning algorithms have been proposed to learn a sparse kernel classifier, most of them fail to bound the number of support vectors used by the final solution which is the average of the intermediate kernel classifiers generated by online algorithms. The key idea of the proposed algorithm is to measure the difficulty in correctly classifying a training example by the derivative of a smooth loss function, and give a more chance to a difficult example to be a support vector than an easy one via a sampling scheme. Our analysis shows that when the loss function is smooth, the proposed algorithm yields similar performance guarantee as the standard online learning algorithm but with a near optimal number of support vectors (up to a poly(lnT) factor). Our empirical study shows promising performance of the proposed algorithm compared to the state-of-the-art algorithms for online sparse kernel learning.}
}

@InProceedings{huang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Spectral Learning of Hidden Markov Models from Dynamic and Static Data},
author = {Huang, Tzu-Kuo and Schneider, Jeff},
pages = {630-638},
abstract = {We develop spectral learning algorithms for Hidden Markov Models  that learn not only from time series, or dynamic data but also  static data drawn independently from the HMM's stationary distribution.  This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and  real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms.}
}

@InProceedings{juhwang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Analogy-preserving Semantic Embedding for Visual Object Categorization},
author = {Ju Hwang, Sung and Grauman, Kristen and Sha, Fei},
pages = {639-647},
abstract = {In multi-class categorization tasks, knowledge about the classes' semantic relationships can provide valuable information beyond the class labels themselves.  However, existing techniques focus on preserving the semantic distances between classes (e.g., according to a given object taxonomy for visual recognition), limiting the influence to pairwise structures.  We propose to model \emph{analogies} that reflect the relationships between multiple pairs of classes simultaneously, in the form ``$p$ is to $q$, as $r$ is to $s$"".  We translate semantic analogies into higher-order geometric constraints called \emph{analogical parallelograms}, and use them in a novel convex regularizer for a discriminatively learned label embedding.   Furthermore, we show how to discover analogies from attribute-based class descriptions, and how to prioritize those likely to reduce inter-class confusion.  Evaluating our Analogy-preserving Semantic Embedding (ASE) on two visual recognition datasets, we demonstrate clear improvements over existing approaches, both in terms of recognition accuracy and analogy completion.  }
}

@InProceedings{izbicki13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Algebraic classifiers: a generic approach to fast cross-validation, online training, and parallel training},
author = {Izbicki, Michael},
pages = {648-656},
abstract = {We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning.  To use these algorithms on a classification model, we must show that the model has appropriate algebraic structure.  It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classifiers and a novel variation of decision stumps called HomStumps.  But not all classifiers have an obvious structure, so we introduce the Free HomTrainer.  This can be used to give a ``generic'' algebraic structure to any classifier.  We use the Free HomTrainer to give algebraic structure to bagging and boosting.  In so doing, we derive novel online and parallel algorithms, and present the first fast cross-validation schemes for these classifiers.}
}

@InProceedings{gupta13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Factorial Multi-Task Learning : A Bayesian Nonparametric Approach},
author = {Gupta, Sunil and Phung, Dinh and Venkatesh, Svetha},
pages = {657-665},
abstract = {Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods.}
}

@InProceedings{gomez-rodriguez13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Modeling Information Propagation with Survival Theory},
author = {Gomez-Rodriguez, Manuel and Leskovec, Jure and Schlkopf, Bernhard},
pages = {666-674},
abstract = {Networks provide a `skeleton' for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.}
}

@InProceedings{dekel13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Better Rates for Any Adversarial Deterministic MDP},
author = {Dekel, Ofer and Hazan, Elad},
pages = {675-683},
abstract = {We consider regret minimization in adversarial deterministic Markov  Decision Processes (ADMDPs) with bandit feedback. We devise a new  algorithm that pushes the state-of-the-art forward in two ways: First,  it attains a regret of $O(T^{2/3})$ with respect to the best fixed  policy in hindsight, whereas the previous best regret bound was  $O(T^{3/4})$. Second, the algorithm and its analysis are compatible  with any feasible ADMDP graph topology, while all previous approaches  required additional restrictions on the graph topology.  }
}

@InProceedings{dimitrakakis13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {ABC Reinforcement Learning},
author = {Dimitrakakis, Christos and Tziortziotis, Nikolaos},
pages = {684-692},
abstract = {  We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows  the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference.  We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.  }
}

@InProceedings{durrant13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Sharp Generalization Error Bounds for Randomly-projected Classifiers},
author = {Durrant, Robert and Kaban, Ata},
pages = {693-701},
abstract = {We derive sharp bounds on the generalization error of a generic linear classifier trained by   empirical risk minimization on randomly-projected data. We make no restrictive assumptions   (such as sparsity or separability) on the data: Instead we use the fact that, in a classification setting, the question of interest is really `what is the effect of random projection on the predicted class labels?' and we therefore derive the exact probability of `label flipping' under Gaussian random projection in order to quantify this effect precisely in our bounds.}
}

@InProceedings{kontorovich13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On learning parametric-output HMMs},
author = {Kontorovich, Aryeh and Nadler, Boaz and Weiss, Roi},
pages = {702-710},
abstract = {We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by {\em decoupling} the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results.}
}

@InProceedings{weinshall13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {LDA Topic Model with Soft Assignment of Descriptors to Words},
author = {Weinshall, Daphna and Levi, Gal and Hanukaev, Dmitri},
pages = {711-719},
abstract = {The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented more naturally by bags of continuous descriptors. Given a finite dictionary of words which are generative models of descriptors, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudo-counts, reflecting the fact that with soft assignment words are not independent.    We use this approach to address novelty detection, where we seek to identify video events with low posterior probability. Video events are described by a generative dynamic texture model, from which we naturally derive a dictionary of generative words.  Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results.}
}

@InProceedings{kamyshanska13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On autoencoder scoring},
author = {Kamyshanska, Hanna and Memisevic, Roland},
pages = {720-728},
abstract = {Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized ``score'' to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier.}
}

@InProceedings{chatzis13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Infinite Markov-Switching Maximum Entropy Discrimination Machines},
author = {Chatzis, Sotirios},
pages = {729-737},
abstract = {In this paper, we present a method that combines the merits of Bayesian  nonparametrics, specifically stick-breaking priors, and large-margin  kernel machines in the context of sequential data classification.  The proposed model postulates a set of (theoretically) infinite interdependent  large-margin classifiers as model components, that robustly capture  local nonlinearity of complex data. The postulated large-margin classifiers  are connected in the context of a Markov-switching construction that  allows for capturing complex temporal dynamics in the modeled datasets.  Appropriate stick-breaking priors are imposed over the component switching  mechanism of our model to allow for data-driven determination of the  optimal number of component large-margin classifiers, under a standard  nonparametric Bayesian inference scheme. Efficient model training  is performed under the maximum entropy discrimination (MED) framework,  which integrates the large-margin principle with Bayesian posterior  inference. We evaluate our method using several real-world datasets,  and compare it to state-of-the-art alternatives.   }
}

@InProceedings{germain13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers},
author = {Germain, Pascal and Habrard, Amaury and Laviolette, Franois and Morvant, Emilie},
pages = {738-746},
abstract = {We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools.  }
}

@InProceedings{papailiopoulos13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Sparse PCA through Low-rank Approximations},
author = {Papailiopoulos, Dimitris and Dimakis, Alexandros and Korokythakis, Stavros},
pages = {747-755},
abstract = {We introduce a novel algorithm that computes the $k$-sparse principal component of a positive semidefinite matrix $A$.  Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of $A$.  We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation.  For example, if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time approximation   algorithm for any desired accuracy.   We implement our algorithm and test it on multiple artificial and real data sets. Due to   a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes.   Our experimental evaluation shows that our scheme is nearly optimal   while finding very sparse vectors.  We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms   in all tested data sets. }
}

@InProceedings{shender13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Computation-Risk Tradeoffs for Covariance-Thresholded Regression},
author = {Shender, Dinah and Lafferty, John},
pages = {756-764},
abstract = {  We present a family of linear regression estimators that provides a fine-grained tradeoff between statistical accuracy and computational efficiency.  The estimators are based on hard thresholding of the sample covariance matrix entries together with l2-regularizion(ridge regression).  We analyze the predictive risk of this family of estimators as a function of the threshold and regularization parameter.  With appropriate parameter choices, the estimate is the solution to a sparse, diagonally dominant linear system, solvable in near-linear time.  Our analysis shows how the risk varies with the sparsity and regularization level, thus establishing a statistical estimation setting for which there is an explicit, smooth tradeoff between risk and computation.  Simulations are provided to support the theoretical analyses.}
}

@InProceedings{malioutov13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Exact Rule Learning via Boolean Compressed Sensing},
author = {Malioutov, Dmitry and Varshney, Kush},
pages = {765-773},
abstract = {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean `sensing' matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting.  We show competitive classification accuracy using the proposed approach.}
}

@InProceedings{chen13h,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Robust Sparse Regression under Adversarial Corruption},
author = {Chen, Yudong and Caramanis, Constantine and Mannor, Shie},
pages = {774-782},
abstract = {We consider high dimensional sparse regression with arbitrary -- possibly, severe or coordinated -- errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error.     We explore the power of a simple idea: replace the essential linear algebraic calculation -- the inner product -- with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust.}
}

@InProceedings{mairal13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Optimization with First-Order Surrogate Functions},
author = {Mairal, Julien},
pages = {783-791},
abstract = {In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions.  First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms.  Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.}
}

@InProceedings{koppula13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation},
author = {Koppula, Hema and Saxena, Ashutosh},
pages = {792-800},
abstract = {We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.}
}

@InProceedings{long13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = { Consistency versus Realizable H-Consistency for Multiclass Classification},
author = {Long, Phil and Servedio, Rocco},
pages = {801-809},
abstract = {A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classification accuracy close to that  of the Bayes optimal classifier. While consistency has been proposed as a  desirable property for multiclass loss functions, we  give experimental and theoretical results exhibiting a  sequence of linearly separable data sources with the following property:  a multiclass classification algorithm which optimizes a loss function  due to Crammer and Singer (which is known not to be consistent) produces  classifiers whose expected error goes to 0, while the expected error  of an algorithm which optimizes a generalization of the loss  function used by LogitBoost (a loss function which is known to be consistent)  is bounded below by a positive constant.    We identify a property of a loss function, realizable  consistency with respect to a restricted class of scoring functions,  that accounts for this difference.   As our main technical results we show  that the Crammer--Singer loss function is  realizable consistent for the class of linear scoring functions, while  the generalization of LogitBoost is not.  Our result for LogitBoost is  a special case of a more general theorem that applies to several other  loss functions that have been proposed for multiclass classification.  }
}

@InProceedings{sabato13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Feature Multi-Selection among Subjective Features},
author = {Sabato, Sivan and Kalai, Adam},
pages = {810-818},
abstract = {When dealing with subjective, noisy, or otherwise nebulous features, the ``wisdom of crowds'' suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated ""feature multi-selection"" algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people's height and weight from photos, using features such as ""gender"" and  ""estimated weight"" as well as culturally fraught ones such as ""attractive"".}
}

@InProceedings{zhang13d,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Domain Adaptation under Target and Conditional Shift},
author = {Zhang, Kun and Schlkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
pages = {819-827},
abstract = {Let $X$ denote the feature and $Y$ the target. We consider domain adaptation under three possible scenarios: (1) the marginal $P_Y$ changes, while the conditional $P_{X|Y}$ stays the same ({\it target shift}), (2) the marginal $P_Y$ is fixed, while the conditional $P_{X|Y}$ changes with certain constraints ({\it conditional shift}), and (3) the marginal $P_{Y}$ changes, and the conditional $P_{X|Y}$ changes with constraints ({\it generalized target shift}). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by {\it reweighting or transforming training data to reproduce the covariate distribution} on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.}
}

@InProceedings{london13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Collective Stability in Structured Prediction: Generalization from One Example},
author = {London, Ben and Huang, Bert and Taskar, Ben and Getoor, Lise},
pages = {828-836},
abstract = {Structured predictors enable joint inference over multiple interdependent output variables. These models are often trained on a small number of examples with large internal structure. Existing distribution-free generalization bounds do not guarantee generalization in this setting, though this contradicts a large body of empirical evidence from computer vision, natural language processing, social networks and other fields. In this paper, we identify a set of natural conditions -- weak dependence, hypothesis complexity and a new measure, collective stability -- that are sufficient for generalization from even a single example, without imposing an explicit generative model of the data. We then demonstrate that the complexity and stability conditions are satisfied by a broad class of models, including marginal inference in templated graphical models. We thus obtain uniform convergence rates that can decrease significantly faster than previous bounds, particularly when each structured example is sufficiently large and the number of training examples is constant, even one.}
}

@InProceedings{raman13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Stable Coactive Learning via Perturbation},
author = {Raman, Karthik and Joachims, Thorsten and Shivaswamy, Pannaga and Schnabel, Tobias},
pages = {837-845},
abstract = {Coactive Learning is a model of interaction between a learning system (e.g. search engine) and its human users, wherein the system learns from (typically implicit) user feedback during operational use. User feedback takes the form of preferences, and recent work has introduced online algorithms that learn from this weak feedback. However, we show that these algorithms can be unstable and ineffective in real-world settings where biases and noise in the feedback are significant. In this paper, we propose the first coactive learning algorithm that can learn robustly despite bias and noise. In particular, we explore how presenting users with slightly perturbed objects (e.g., rankings) can stabilize the learning process. We theoretically validate the algorithm by proving bounds on the average regret. We also provide extensive empirical evidence on benchmarks and from a live search engine user study, showing that the new algorithm substantially outperforms existing methods.}
}

@InProceedings{wang13d,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Max-Margin Multiple-Instance Dictionary Learning},
author = {Wang, Xinggang and Wang, Baoyuan and Bai, Xiang and Liu, Wenyu and Tu, Zhuowen},
pages = {846-854},
abstract = {Dictionary learning has became an increasingly important task in machine learning, as it is fundamental to the representation problem. A number of emerging techniques specifically include a codebook learning step, in which a critical knowledge abstraction process is carried out. Existing approaches in dictionary (codebook) learning are either generative (unsupervised e.g. k-means) or discriminative (supervised e.g. extremely randomized forests). In this paper, we propose a multiple instance learning (MIL) strategy (along the line of weakly supervised learning) for dictionary learning. Each code is represented by a classifier, such as a linear SVM, which naturally performs metric fusion for multi-channel features. We design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in MIL.  State-of-the-art results are observed in image classification benchmarks based on the learned codebooks, which observe both compactness and effectiveness.}
}

@InProceedings{iyer13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fast Semidifferential-based Submodular Function Optimization},
author = {Iyer, Rishabh and Jegelka, Stefanie and Bilmes, Jeff},
pages = {855-863},
abstract = {We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization.  Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning.  We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.}
}

@InProceedings{gonen13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Kernelized Bayesian Matrix Factorization},
author = {Gönen, Mehmet and Khan, Suleiman and Kaski, Samuel},
pages = {864-872},
abstract = {We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms.}
}

@InProceedings{gens13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning the Structure of Sum-Product Networks},
author = {Gens, Robert and Pedro, Domingos},
pages = {873-880},
abstract = {Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.}
}

@InProceedings{yang13f,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Quantile Regression for Large-scale Applications},
author = {Yang, Jiyan and Meng, Xiangrui and Mahoney, Michael},
pages = {881-887},
abstract = {Quantile regression is a method to estimate the quantiles of the   conditional distribution of a response variable, and as such it permits a   much more accurate portrayal of the relationship between the response variable   and observed covariates than methods such as Least-squares or   Least Absolute Deviations regression.  It can be expressed as a linear program,   and   interior-point methods can be used to find a solution for  moderately large problems.  Dealing with very large problems, \emph{e.g.}, involving data up to and   beyond the terabyte regime, remains a challenge.  Here, we present a randomized algorithm that runs in time that is nearly   linear in the size of the input and that, with constant probability,   computes a $(1+\epsilon)$ approximate solution to an arbitrary quantile   regression problem.  Our algorithm computes a low-distortion subspace-preserving  embedding with respect to the loss function of quantile regression.  Our empirical evaluation illustrates that our algorithm is competitive with   the best previous work on small to medium-sized problems, and that   it can be implemented in MapReduce-like environments and    applied to terabyte-sized problems.}
}

@InProceedings{meng13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Robust Regression on MapReduce},
author = {Meng, Xiangrui and Mahoney, Michael},
pages = {888-896},
abstract = {Although the MapReduce framework is now the \emph{de facto} standard for   analyzing massive data sets, many algorithms (in particular, many   iterative algorithms popular in machine learning, optimization, and linear   algebra) are hard to fit into MapReduce.   Consider, \emph{e.g.}, the $\ell_p$ regression problem: given a matrix   $A \in \mathbb{R}^{m \times n}$ and a vector $b \in \mathbb{R}^m$, find a   vector $x^* \in \mathbb{R}^n$ that minimizes $f(x) = \|A x - b\|_p$.   The widely-used $\ell_2$ regression, \emph{i.e.}, linear least-squares, is   known to be highly sensitive to outliers; and choosing $p \in [1, 2)$ can   help improve robustness.  In this work, we propose an efficient algorithm for solving strongly   over-determined $(m \gg n)$ robust $\ell_p$ regression problems to moderate precision on   MapReduce.  Our empirical results on data up to the terabyte scale demonstrate   that our algorithm is a significant improvement over traditional iterative algorithms on MapReduce   for $\ell_1$ regression, even for a fairly small number   of iterations.   In addition, our proposed interior-point cutting-plane method can also be   extended to solving more general convex problems on MapReduce.}
}

@InProceedings{ogawa13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines},
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
pages = {897-905},
abstract = {The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches.  }
}

@InProceedings{gao13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {One-Pass AUC Optimization},
author = {Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
pages = {906-914},
abstract = {AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.}
}

@InProceedings{jancsary13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Convex QP Relaxations for Structured Prediction},
author = {Jancsary, Jeremy and Nowozin, Sebastian and Rother, Carsten},
pages = {915-923},
abstract = {We introduce a new large margin approach to discriminative training of intractable discrete graphical models. Our approach builds on a convex quadratic programming relaxation of the MAP inference problem. The model parameters are trained directly within this restricted class of energy functions so as to optimize the predictions on the training data. We address the issue of how to parameterize the resulting model and point out its relation to existing approaches. The primary motivation behind our use of the QP relaxation is its computational efficiency; yet, empirically, its predictive accuracy compares favorably to more expensive approaches. This makes it an appealing choice for many practical tasks.}
}

@InProceedings{silver13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Concurrent Reinforcement Learning from Customer Interactions},
author = {Silver, David and Newnham, Leonard and Barker, David and Weller, Suzanne and McFall, Jason},
pages = {924-932},
abstract = {In this paper, we explore applications in which a company interacts concurrently with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for concurrent reinforcement learning, using a variant of temporal-difference learning to learn efficiently from partial interaction sequences.   We evaluate our algorithms in two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. }
}

@InProceedings{sun13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner},
author = {Sun, Peng and Zhou, Jie},
pages = {933-941},
abstract = {For a well trained Boosting classifier, we are interested in how to save the testing time, i.e., to make the decision without evaluating all the base learners. To address this problem, in previous work the base learners are sequentially calculated and early stopping is allowed if the decision function has been confident enough to output its value. In such a chain structure, the order of base learners is critical: better order can lead to less evaluation time.    In this paper, we present a novel method for ordering. We base our discussion on the data structure representing Boosting's decision function. Viewing the decision function a boolean expression, we propose a Binary Valued Tree for its representation. As a secondary contribution, such a representation unifies the work by previous researchers and helps devise new representation. Also, its connection to Binary Decision Diagram(BDD) is discussed.}
}

@InProceedings{kuzborskij13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Stability and Hypothesis Transfer Learning},
author = {Kuzborskij, Ilja and Orabona, Francesco},
pages = {942-950},
abstract = {We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it -- the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.}
}

@InProceedings{emtiyazkhan13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models},
author = {Emtiyaz Khan, Mohammad and Aravkin, Aleksandr and Friedlander, Michael and Seeger, Matthias},
pages = {951-959},
abstract = {Latent Gaussian models (LGMs) are widely used in statistics and machine learning.  Bayesian inference in non-conjugate LGM is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods.  Algorithms based on Variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use.  However, the structure of optimization problems associated with them    remains poorly understood, and standard solvers take too long to converge.  In this paper, we derive a novel dual variational inference approach, which exploits the convexity property of the VG approximations.   The implications of our approach is that we obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods.  Using real world data, we demonstrate these advantages on a variety of LGMs including Gaussian process classification and latent Gaussian Markov random fields.    }
}

@InProceedings{herlau13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Modeling Temporal Evolution and Multiscale Structure in Networks},
author = {Herlau, Tue and Mrup, Morten and Schmidt, Mikkel},
pages = {960-968},
abstract = {Many real-world networks exhibit both temporal evolution and multiscale structure.  We propose a model for temporally correlated multifurcating hierarchies in complex networks which jointly capture both effects. We use the Gibbs fragmentation tree as prior over multifurcating trees and a change-point model to account for the temporal evolution of each vertex.  We demonstrate that our model is able to infer time-varying multiscale structure in synthetic as well as three real world time-evolving complex networks.  Our modeling of the temporal evolution of hierarchies brings new insights into the changing roles and position of entities and possibilities for better understanding these dynamic complex systems.  }
}

@InProceedings{chen13i,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Dependent Normalized Random Measures},
author = {Chen, Changyou and Rao, Vinayak and Buntine, Wray and Whye Teh, Yee},
pages = {969-977},
abstract = {In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modelling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process.}
}

@InProceedings{xu13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fast Max-Margin Matrix Factorization with Data Augmentation},
author = {Xu, Minjie and Zhu, Jun and Zhang, Bo},
pages = {978-986},
abstract = {Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.}
}

@InProceedings{gupta13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Natural Image Bases to Represent Neuroimaging Data},
author = {Gupta, Ashish and Ayhan, Murat and Maida, Anthony},
pages = {987-994},
abstract = {Visual inspection of neuroimagery is susceptible to human eye limitations.  Computerized methods have been shown to be equally or more effective   than human clinicians in diagnosing dementia from neuroimages. Nevertheless,   much of the work involves the use of domain expertise to extract hand-crafted features. The key technique in this paper is the use of cross-domain features to represent MRI data.  We used a sparse autoencoder to learn a set of bases from natural images and   then applied convolution to extract features from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset.Using this new representation, we classify MRI instances into three categories: Alzheimer's Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC).Our approach, in spite of being very simple, achieved high classification performance,   which is competitive with or better than other approaches.}
}

@InProceedings{ailon13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Breaking the Small Cluster Barrier of Graph Clustering},
author = {Ailon, Nir and Chen, Yudong and Xu, Huan},
pages = {995-1003},
abstract = {This paper investigates graph clustering in the planted cluster model in the   presence of  {\em small clusters}. Traditional results dictate that for an   algorithm to provably correctly recover the clusters, {\em all} clusters must be   sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ is the number   of nodes of the graph). We show that this is not really a restriction: by a more refined   analysis of the trace-norm based matrix recovery approach proposed in (Jalali et al. 2011) and (Chen et al. 2012), we prove that small clusters, under certain mild assuptions, do not hinder recovery of large ones.  Based on this result, we further devise an iterative algorithm   to recover {\em almost all clusters} via a ``peeling strategy'', i.e., recover large clusters   first, leading to a reduced problem, and repeat this procedure.   These results are extended to the    {\em partial observation} setting, in which only a (chosen) part of the graph is observed.  The peeling strategy gives rise to an active learning algorithm, in which   edges adjacent to smaller clusters are queried more often as large clusters are learned  (and removed).  Our findings are supported by experiments.    From a high level, this paper sheds novel insights on high-dimesional statistics and   learning structured data, by presenting a structured matrix learning problem for which  a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations  does the job.}
}

@InProceedings{sheldon13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Approximate Inference in Collective Graphical Models},
author = {Sheldon, Daniel and Sun, Tao and Kumar, Akshat and Dietterich, Tom},
pages = {1004-1012},
abstract = {We study the problem of approximate inference in collective graphical models (CGMs), which were recently introduced to model the problem of learning and inference with noisy aggregate observations. We first analyze the complexity of inference in CGMs: unlike inference in conventional graphical models, exact inference in CGMs is NP-hard even for tree-structured models. We then develop a tractable convex approximation to the NP-hard MAP inference problem in CGMs, and show how to use MAP inference for approximate marginal inference within the EM framework. We demonstrate empirically that these approximation techniques can reduce the computational cost of inference by two orders of magnitude and the cost of learning by at least an order of magnitude while providing solutions of equal or better quality.}
}

@InProceedings{reed13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Scaling the Indian Buffet Process via Submodular Maximization},
author = {Reed, Colorado and Zoubin, Ghahramani},
pages = {1013-1021},
abstract = {Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Wellings (2008)'s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3-approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model. }
}

@InProceedings{takac13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Mini-Batch Primal and Dual Methods for SVMs},
author = {Takac, Martin and Bijral, Avleen and Richtarik, Peter and Srebro, Nati},
pages = {1022-1030},
abstract = {We address the issue of using mini-batches in stochastic  optimization of SVMs. We show that the same quantity, the  spectral norm of the data, controls the parallelization  speedup obtained for both primal stochastic subgradient descent(SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.}
}

@InProceedings{homrighausen13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The lasso, persistence, and cross-validation},
author = {Homrighausen, Darren and McDonald, Daniel},
pages = {1031-1039},
abstract = {During the last fifteen years, the lasso procedure has been the target of a substantial amount of theoretical and applied research. Correspondingly, many results are known about its behavior for a fixed or optimally chosen smoothing parameter (given up to unknown constants). Much less, however, is known about the lasso's behavior when the smoothing parameter is chosen in a data dependent way. To this end, we give the first result about the risk consistency of lasso when the smoothing parameter is chosen via cross-validation. We consider the high-dimensional setting wherein the number of predictors $p=n^\alpha,\ \alpha>0$ grows with the number of observations.  }
}

@InProceedings{tejasvichaganty13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Spectral Experts for Estimating Mixtures of Linear Regressions},
author = {Tejasvi Chaganty, Arun and Liang, Percy},
pages = {1040-1048},
abstract = {Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima.  In this paper, we develop a new computationally efficient and provably consistent estimator for the mixture of linear regressions, a simple instance of discriminative latent-variable models.  Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using the tensor power method.  We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM).  }
}

@InProceedings{oliva13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Distribution to Distribution Regression},
author = {Oliva, Junier and Poczos, Barnabas and Schneider, Jeff},
pages = {1049-1057},
abstract = {We analyze 'Distribution to Distribution regression' where one is regressing a mapping where both the covariate (inputs) and response (outputs) are distributions. No parameters on the input or output distributions are assumed, nor are any strong assumptions made on the measure from which input distributions are drawn from. We develop an estimator and derive an upper bound for the $L2$ risk; also, we show that when the effective dimension is small enough (as measured by the doubling dimension), then the risk converges to zero with a polynomial rate.}
}

@InProceedings{wan13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Regularization of Neural Networks using DropConnect},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
pages = {1058-1066},
abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}

@InProceedings{wilson13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
author = {Wilson, Andrew and Adams, Ryan},
pages = {1067-1075},
abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}

@InProceedings{xu13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Anytime Representation Learning},
author = {Xu, Zhixiang and Kusner, Matt and Huang, Gao and Weinberger, Kilian},
pages = {1076-1084},
abstract = {Evaluation cost during test-time is becoming increasingly important as many real-world applications need fast evaluation (e.g. web search engines, email spam filtering) or use expensive features (e.g. medical diagnosis). We introduce Anytime Feature Representations (AFR), a novel algorithm that explicitly addresses this trade-off in the data representation rather than in the classifier. This enables us to turn conventional classifiers, in particular Support Vector Machines, into test-time cost sensitive anytime classifiers combining the advantages of anytime learning and large-margin classification.}
}

@InProceedings{nguyen13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Algorithms for Direct 0--1 Loss Optimization in Binary Classification},
author = {Nguyen, Tan and Sanner, Scott},
pages = {1085-1093},
abstract = {While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers.  On the other hand, while the non-convex 0--1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice.  In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0--1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0--1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0--1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers.  To this end, we believe this work reiterates the importance of 0--1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.}
}

@InProceedings{busa-fekete13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Top-k Selection based on Adaptive Sampling of Noisy Preferences},
author = {Busa-Fekete, Robert and Szorenyi, Balazs and Cheng, Weiwei and Weng, Paul and Huellermeier, Eyke},
pages = {1094-1102},
abstract = {We consider the problem of reliably selecting an optimal subset of fixed size from a given set of choice alternatives, based on noisy information about the quality of these alternatives. Problems of similar kind have been tackled by means of adaptive sampling schemes called racing algorithms. However, in contrast to existing approaches, we do not assume that each alternative is characterized by a real-valued random variable, and that samples are taken from the corresponding distributions. Instead, we only assume that alternatives can be compared in terms of pairwise preferences. We propose and formally analyze a general preference-based racing algorithm that we instantiate with three specific ranking procedures and corresponding sampling schemes. Experiments with real and synthetic data are presented to show the efficiency of our approach.}
}

@InProceedings{bugraerol13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The Extended Parameter Filter},
author = {Bugra Erol, Yusuf and Li, Lei and Ramsundar, Bharath and Stuart, Russell},
pages = {1103-1111},
abstract = {The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant.  In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.}
}

@InProceedings{balcan13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Exploiting Ontology Structures and Unlabeled Data for Learning},
author = {Balcan, Nina and Blum, Avrim and Mansour, Yishay},
pages = {1112-1120},
abstract = {We present and analyze a theoretical model designed to understand and  explain the effectiveness of ontologies for learning multiple related  tasks from primarily unlabeled data.  We present both  information-theoretic results as well as efficient algorithms.    We show in this model that an ontology, which specifies the  relationships between multiple outputs, in some cases is sufficient  to completely learn a classification using a large unlabeled data  source.}
}

@InProceedings{zhang13e,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions},
author = {Zhang, Lijun and Yang, Tianbao and Jin, Rong and He, Xiaofei},
pages = {1121-1129},
abstract = {Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result.}
}

@InProceedings{dembczynski13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization},
author = {Dembczynski, Krzysztof and Jachnik, Arkadiusz and Kotlowski, Wojciech and Waegeman, Willem and Huellermeier, Eyke},
pages = {1130-1138},
abstract = {We compare the plug-in rule approach for optimizing the F-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F-measure directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm.}
}

@InProceedings{sutskever13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On the importance of initialization and momentum in deep learning},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
pages = {1139-1147},
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }
}

@InProceedings{georgiev13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines},
author = {Georgiev, Kostadin and Nakov, Preslav},
pages = {1148-1156},
abstract = {We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBM-based approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previously-proposed approaches.  }
}

@InProceedings{richard13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Intersecting singularities for multi-structured estimation},
author = {Richard, Emile and BACH, Francis and Vert, Jean-Philippe},
pages = {1157-1165},
abstract = {We address the problem of designing a convex nonsmooth regularizer encouraging multiple structural effects simultaneously. Focusing on the inference of sparse and low-rank matrices we suggest a new complexity index and a convex penalty approximating it. The new penalty term can be written as the trace norm of a linear function of the matrix. By analyzing theoretical properties of this family of regularizers  we come up with oracle inequalities and compressed sensing results ensuring the quality of our regularized estimator. We also provide algorithms and supporting numerical experiments.}
}

@InProceedings{duvenaud13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
author = {Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},
pages = {1166-1174},
abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.}
}

@InProceedings{friedland13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events},
author = {Friedland, Lisa and Jensen, David and Lavine, Michael},
pages = {1175-1183},
abstract = {In this paper, we analyze the task of inferring rare links between pairs of entities that seem too similar to have occurred by chance. Variations of this task appear in such diverse areas as social network analysis, security, fraud detection, and entity resolution. To address the task in a general form, we propose a simple, flexible mixture model in which most entities are generated independently from a distribution but a small number of pairs are constrained to be similar. We predict the true pairs using a likelihood ratio that trades off the entities' similarity with their rarity. This method always outperforms using only similarity; however, with certain parameter settings, similarity turns out to be surprisingly competitive. Using real data, we apply the model to detect twins given their birth weights and to re-identify cell phone users based on distinctive usage patterns.}
}

@InProceedings{grunewalder13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Smooth Operators},
author = {Grunewalder, Steffen and Arthur, Gretton and Shawe-Taylor, John},
pages = {1184-1192},
abstract = {We develop a generic approach to form smooth versions of basic mathematical operations like multiplication, composition, change of measure, and conditional expectation, among others.   Operations which result in functions outside the reproducing kernel Hilbert space (such as the product of two RKHS functions) are approximated via a natural cost function, such that the solution is guaranteed to be in the targeted RKHS. This approximation problem is reduced to a regression problem using an adjoint trick, and solved in a vector-valued RKHS, consisting of continuous, linear, smooth operators which map from an input, real-valued RKHS to the desired target RKHS. Important constraints, such as an almost everywhere positive density, can be enforced or approximated naturally in this framework, using convex constraints on the operators. Finally, smooth operators can be composed to accomplish more complex machine learning tasks, such as the sum rule and kernelized approximate Bayesian inference, where state-of-the-art convergence rates are obtained.}
}

@InProceedings{goschin13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The Cross-Entropy Method Optimizes for Quantiles},
author = {Goschin, Sergiu and Weinstein, Ari and Littman, Michael},
pages = {1193-1201},
abstract = {Cross-entropy optimization (CE) has proven to be a powerful tool for search in control environments. In the basic scheme, a distribution over proposed solutions is repeatedly adapted by evaluating a sample of solutions and refocusing the distribution on a percentage of those with the highest scores.  We show that, in the kind of noisy evaluation environments that are common in decision-making domains, this percentage-based refocusing does not optimize the expected utility of solutions, but instead a quantile metric. We provide a variant of CE (Proportional CE) that effectively optimizes the expected value. We show using variants of established noisy environments that Proportional CE can be used in place of CE and can improve solution quality.}
}

@InProceedings{ding13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Topic Discovery through Data Dependent and Random Projections},
author = {Ding, Weicong and Hossein Rohban, Mohammad and Ishwar, Prakash and Saligrama, Venkatesh},
pages = {1202-1210},
abstract = {We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and realworld datasets to demonstrate qualitative and quantitative merits of our scheme.}
}

@InProceedings{bellemare13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Bayesian Learning of Recursively Factored Environments},
author = {Bellemare, Marc and Veness, Joel and Bowling, Michael},
pages = {1211-1219},
abstract = {Model-based reinforcement learning techniques have historically encountered a number of difficulties scaling up to large observation spaces. One promising approach has been to decompose the model learning task into a number of smaller, more manageable sub-problems by factoring the observation space. Typically, many different factorizations are possible, which can make it difficult to select an appropriate factorization without extensive testing. In this paper we introduce the class of recursively decomposable factorizations, and show how exact Bayesian inference can be used to efficiently guarantee predictive performance close to the best factorization in this class. We demonstrate the strength of this approach by presenting a collection of empirical results for 20 different Atari 2600 games.  }
}

@InProceedings{agarwal13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Selective sampling algorithms for cost-sensitive multiclass prediction},
author = {Agarwal, Alekh},
pages = {1220-1228},
abstract = {In this paper, we study the problem of active learning for cost-sensitive multiclass classification. We propose selective  sampling algorithms, which process the data in a streaming fashion,  querying only a subset of the labels. For these algorithms, we analyze the regret and label complexity when the labels are generated according to a generalized linear model. We establish that the gains of active learning over passive learning can range from none to exponentially large, based on a natural notion of margin. We also  present a safety guarantee to guard against model mismatch. Numerical  simulations show that our algorithms indeed obtain a low regret with a  small number of queries.}
}

@InProceedings{kalaitzis13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {The Bigraphical Lasso},
author = {Kalaitzis, Alfredo and Lafferty, John and Lawrence, Neil and Zhou, Shuheng},
pages = {1229-1237},
abstract = {The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions.}
}

@InProceedings{karnin13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Almost Optimal Exploration in Multi-Armed Bandits},
author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
pages = {1238-1246},
abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.  }
}

@InProceedings{andrew13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Deep Canonical Correlation Analysis},
author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
pages = {1247-1255},
abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emph{canonical correlation analysis} (CCA).  It is an alternative to the nonparametric method \emph{kernel canonical correlation analysis} (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}

@InProceedings{denil13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Consistency of Online Random Forests},
author = {Denil, Misha and Matheson, David and Freitas  Nando, De},
pages = {1256-1264},
abstract = {As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests.}
}

@InProceedings{wytock13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting},
author = {Wytock, Matt and Kolter, Zico},
pages = {1265-1273},
abstract = {This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs. The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems.  In this paper, we make three contributions: 1) we develop a second-order active-set method which is several orders of magnitude faster that previously proposed optimization approaches for this problem 2) we analyze the model from a theoretical standpoint, improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and 3) we apply the method to large-scale energy forecasting problems, demonstrating state-of-the-art performance on two real-world tasks.}
}

@InProceedings{chen13j,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fast Image Tagging},
author = {Chen, Minmin and Zheng, Alice and Weinberger, Kilian},
pages = {1274-1282},
abstract = {Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.}
}

@InProceedings{tesch13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Expensive Function Optimization with Stochastic Binary Outcomes},
author = {Tesch, Matthew and Schneider, Jeff and Choset, Howie},
pages = {1283-1291},
abstract = {Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome -- success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training.  In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem.  We propose an experiment selection metric for this setting based on expected improvement.  We demonstrate the algorithm's performance on synthetic problems and on a real snake robot learning to move over an obstacle.}
}

@InProceedings{geras13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Multiple-source cross-validation},
author = {Geras, Krzysztof and Sutton, Charles},
pages = {1292-1300},
abstract = {Cross-validation is an essential tool in machine learning and statistics. The typical procedure, in which data points are randomly assigned to one of the test sets, makes an implicit assumption that the data are exchangeable. A common case in which this does not hold is when the data come from multiple sources, in the sense used in transfer learning. In this case it is common to arrange the cross-validation procedure in a way that takes the source structure into account. Although common in practice, this procedure does not appear to have been theoretically analysed. We present new estimators of the variance of the cross-validation, both in the multiple-source setting and in the standard iid setting. These new estimators allow for much more accurate confidence intervals and hypothesis tests to compare algorithms.}
}

@InProceedings{zhou13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Triggering Kernels for Multi-dimensional Hawkes Processes},
author = {Zhou, Ke and Zha, Hongyuan and Song, Le},
pages = {1301-1309},
abstract = {  How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi-dimensional Hawkes processes. In particular, we focus on the nonparametric learning of the  triggering kernels, and propose an algorithm {\sf MMEL} that combines the idea of decoupling the parameters through constructing a tight upper-bound of the objective function and application of Euler-Lagrange equations for optimization in infinite dimensional functional space.    We show that the proposed method performs significantly better than alternatives in experiments on both synthetic and real world datasets.  }
}

@InProceedings{pascanu13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On the difficulty of training recurrent neural networks},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
pages = {1310-1318},
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  }
}

@InProceedings{goodfellow13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Maxout Networks},
author = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
pages = {1319-1327},
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We dene a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropouts fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classication performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.  }
}

@InProceedings{rastegari13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Predictable Dual-View Hashing},
author = {Rastegari, Mohammad and Choi, Jonghyun and Fakhraei, Shobeir and Hal, Daume and Davis, Larry},
pages = {1328-1336},
abstract = {We propose a Predictable Dual-View Hashing (PDH) algorithm which embeds proximity of data samples in the original spaces. We create a cross-view hamming space with the ability to compare information from previously incomparable domains with a notion of `predictability'.   By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms.}
}

@InProceedings{coates13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Deep learning with COTS HPC systems},
author = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Catanzaro, Bryan and Andrew, Ng},
pages = {1337-1345},
abstract = {Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features.  Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores.  In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines.  As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.}
}

@InProceedings{ross13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Nonparametric Mixture of Gaussian Processes with Constraints},
author = {Ross, James and Dy, Jennifer},
pages = {1346-1354},
abstract = {Motivated by the need to identify new and clinically relevant categories of lung disease, we propose a novel clustering with constraints method using a Dirichlet process mixture of Gaussian processes in a variational Bayesian nonparametric framework. We claim that individuals should be grouped according to biological and/or genetic similarity regardless of their level of disease severity; therefore, we introduce a new way of looking at subtyping/clustering by recasting it in terms of discovering associations of individuals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). The nonparametric nature of our algorithm allows for learning the unknown number of meaningful trajectories. Additionally, we acknowledge the usefulness of expert guidance by providing for their input using must-link and cannot- link constraints. These constraints are encoded with Markov random fields. We also provide an efficient variational approach for performing inference on our model.}
}

@InProceedings{jreddi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Scale Invariant Conditional Dependence Measures},
author = {J Reddi, Sashank and Poczos, Barnabas},
pages = {1355-1363},
abstract = {In this paper we develop new dependence and conditional dependence measures and provide their estimators. An attractive property of these measures and estimators is that they are invariant to any monotone increasing transformations of the random variables, which is important in many applications including feature selection. Under certain conditions we show the consistency of these estimators, derive upper bounds on their convergence rates, and show that the estimators do not suffer from the curse of dimensionality. However, when the conditions are less restrictive, we derive a lower bound which proves that in the worst case the convergence can be arbitrarily slow similarly to some other estimators. Numerical illustrations demonstrate the applicability of our method.}
}

@InProceedings{ross13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning Policies for Contextual Submodular Prediction},
author = {Ross, Stephane and Zhou, Jiaji and Yue, Yisong and Dey, Debadeepta and Bagnell, Drew},
pages = {1364-1372},
abstract = {Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.}
}

@InProceedings{kim13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Manifold Preserving Hierarchical Topic Models for Quantization and Approximation},
author = {Kim, Minje and Smaragdis, Paris},
pages = {1373-1381},
abstract = {We present two complementary topic models to address the analysis of mixture data lying on manifolds. First, we propose a quantization method with an additional mid-layer latent variable, which selects only data points that best preserve the manifold structure of the input data. In order to address the case of modeling all the in-between parts of that manifold using this reduced representation of the input, we introduce a new model that provides a manifold-aware interpolation method. We demonstrate the advantages of these models with experiments on the hand-written digit recognition and the speech source separation tasks.}
}

@InProceedings{ogawa13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Safe Screening of Non-Support Vectors in Pathwise SVM Computation},
author = {Ogawa, Kohei and Suzuki, Yoshiki and Takeuchi, Ichiro},
pages = {1382-1390},
abstract = { In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost.}
}

@InProceedings{avilapires13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Cost-sensitive Multiclass Classification Risk Bounds},
author = {Ávila Pires, Bernardo and Szepesvari, Csaba and Ghavamzadeh, Mohammad},
pages = {1391-1399},
abstract = {A commonly used approach to multiclass classification is to replace the $0-1$ loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the $0-1$ excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of  Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced ``Simplex Coding'' scheme.}
}

@InProceedings{yi13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Semi-supervised Clustering by Input Pattern Assisted Pairwise Similarity Matrix Completion},
author = {Yi, Jinfeng and Zhang, Lijun and Jin, Rong and Qian, Qi and Jain, Anil},
pages = {1400-1408},
abstract = {Many semi-supervised clustering algorithms have been proposed to improve the clustering accuracy by effectively exploring the available side information that is usually in the form of pairwise constraints. Despite the progress, there are two main shortcomings of the existing semi-supervised clustering algorithms. First, they have to deal with non-convex optimization problems, leading to clustering results that are sensitive to the initialization. Second, none of these algorithms is equipped with theoretical guarantee regarding the clustering performance. We address these limitations by developing a framework for semi-supervised clustering based on {\it input pattern assisted matrix completion}. The key idea is to cast clustering into a matrix completion problem, and solve it efficiently by exploiting the correlation between input patterns and cluster assignments. Our analysis shows that under appropriate conditions, only $O(\log n)$ pairwise constraints are needed to accurately recover the true cluster partition. We verify the effectiveness of the proposed algorithm by comparing it to the state-of-the-art semi-supervised clustering algorithms on several benchmark datasets.}
}

@InProceedings{simsekli13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Learning the beta-Divergence in Tweedie Compound Poisson Matrix Factorization Models},
author = {Simsekli, Umut and Taylan  Cemgil, Ali and Kenan Yilmaz, Yusuf},
pages = {1409-1417},
abstract = {In this study, we derive algorithms for estimating mixed $\beta$-divergences. Such cost functions are useful for Nonnegative Matrix and Tensor Factorization models with a compound Poisson observation model. Compound Poisson is a particular Tweedie model, an important special case of exponential dispersion models characterized by the fact that the variance is proportional to a power function of the mean. There are several well known matrix and tensor factorization algorithms that minimize the $\beta$-divergence; these estimate the mean parameter. The probabilistic interpretation gives us more flexibility and robustness by providing us additional tunable parameters such as power and dispersion. Estimation of the power parameter is useful for choosing a suitable divergence and estimation of dispersion is useful for data driven regularization and weighting in collective/coupled factorization of heterogeneous datasets. We present three inference algorithms for both estimating the factors and the additional parameters of the compound Poisson distribution. The methods are evaluated on two applications: modeling symbolic representations for polyphonic music and lyric prediction from audio features. Our conclusion is that the compound poisson based factorization models can be useful for sparse positive data.}
}

@InProceedings{kuleshov13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration},
author = {Kuleshov, Volodymyr},
pages = {1418-1425},
abstract = {We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with $k$ non-zero elements at a cost of $O(nk + k^3)$ flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be interpreted as a type of second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data.}
}

@InProceedings{ahmed13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Nested Chinese Restaurant Franchise Process:  Applications to User Tracking and Document Modeling},
author = {Ahmed, Amr and Hong, Liangjie and Smola, Alexander},
pages = {1426-1434},
abstract = {Much natural data is hierarchical in nature. Moreover, this hierarchy  is often shared between different instances. We introduce the  nested Chinese Restaurant Franchise Process as a means to obtain both  hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin  to the Hierarchical Dirichlet Process.     Moreover, by decoupling the \emph{structure generating} part of the  process from the components responsible for the observations, we are  able to apply the same statistical approach to a variety of user  generated data. In particular, we model the joint distribution of  microblogs and locations for Twitter for users. This leads to a 40\%  reduction in location uncertainty relative to the best previously  published results. Moreover, we model documents from the NIPS papers  dataset, obtaining excellent perplexity relative to (hierarchical)  Pachinko allocation and LDA.}
}

@InProceedings{curtin13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Tree-Independent Dual-Tree Algorithms},
author = {Curtin, Ryan and March, William and Ram, Parikshit and Anderson, David and Gray, Alexander and Isbell, Charles},
pages = {1435-1443},
abstract = {Dual-tree algorithms are a widely used class of branch-and-bound algorithms.  Unfortunately, developing dual-tree algorithms for use with different trees and problems is often complex and burdensome.  We introduce a four-part logical split: the tree, the traversal, the point-to-point base case, and the pruning rule.  We provide a meta-algorithm which allows development of dual-tree algorithms in a tree-independent manner and easy extension to entirely new types of trees.  Representations are provided  for five common algorithms; for k-nearest neighbor search, this leads to a novel, tighter pruning bound. The meta-algorithm also allows  straightforward extensions to massively parallel settings.}
}

@InProceedings{romera-paredes13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Multilinear Multitask Learning},
author = {Romera-Paredes, Bernardino and Aung, Hane and Bianchi-Berthouze, Nadia and Pontil, Massimiliano},
pages = {1444-1452},
abstract = {Many real world datasets occur or can be arranged into multi-modal structures.  With such datasets, the tasks to be learnt can be referenced by multiple indices.  Current multitask learning frameworks are not designed to account for the preservation of this information.  We propose the use of multilinear algebra as a natural way to model such a set of related tasks. We present two learning methods;   one is an adapted convex relaxation method used in the context of tensor completion.  The second method is based on the Tucker decomposition and on alternating minimization. Experiments on synthetic and real data indicate that the multilinear approaches provide a significant  improvement over other multitask learning methods. Overall our second  approach yields the best performance in all datasets.}
}

@InProceedings{joulani13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Online Learning under Delayed Feedback},
author = {Joulani, Pooria and Gyorgy, Andras and Szepesvari, Csaba},
pages = {1453-1461},
abstract = {Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.}
}

@InProceedings{wang13e,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Adaptive Hamiltonian and Riemann Manifold Monte Carlo},
author = {Wang, Ziyu and Mohamed, Shakir and Freitas  Nando, De},
pages = {1462-1470},
abstract = {In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and demonstrate on several models and data sets that the use of our adaptive algorithms makes it is easy to obtain more efficient samplers, in some precluding the need for more complex models. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.}
}

@InProceedings{sodomka13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {Coco-Q: Learning in Stochastic Games with Side Payments},
author = {Sodomka, Eric and Hilliard, Elizabeth and Littman, Michael and Greenwald, Amy},
pages = {1471-1479},
abstract = {Coco (""cooperative/competitive"") values are a solution concept for two-player normal-form games with transferable utility, when binding agreements and side payments between players are possible. In this paper, we show that coco values can also be defined for stochastic games and can be learned using a simple variant of Q-learning that is provably convergent. We provide a set of examples showing how the strategies learned by the Coco-Q algorithm relate to those learned by existing multiagent Q-learning algorithms.}
}

@InProceedings{ho13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {3},
section = {cycle-3},
month = {May},
year = {2013},
title = {On A Nonlinear Generalization of Sparse Coding and Dictionary Learning},
author = {Ho, Jeffrey and Xie, Yuchen and Vemuri, Baba},
pages = {1480-1488},
abstract = {Existing dictionary learning algorithms are based on the  assumption that the data are vectors in an Euclidean vector  space, and the dictionary is learned from the training data using the vector space structure and its Euclidean metric. However, in many applications, features and data often originated from a Riemannian manifold that does not support a global linear (vector space) structure.  Furthermore, the extrinsic viewpoint of existing dictionary learning algorithms becomes inappropriate for modeling and incorporating the intrinsic geometry of the manifold that is potentially important and critical to the application. This paper proposes a novel framework for sparse coding and dictionary learning for data on a Riemannian manifold, and it shows that  the existing sparse coding and dictionary learning methods can be considered as special (Euclidean) cases of the more general framework proposed here. We show that both the dictionary and sparse coding can  be effectively computed for several important classes of Riemannian manifolds, and we validate the proposed method using two well-known classification problems in computer vision and medical imaging analysis.}
}

