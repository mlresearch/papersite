@Proceedings{ICML-2013,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
month = {May},
year = {2013}}


@InProceedings{yang13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Mixture of Mutually Exciting Processes for Viral Diffusion},
author = {Yang, Shuang-Hong and Zha, Hongyuan},
pages = {1-9},
abstract = {\emph{Diffusion network inference} and \emph{meme tracking} have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web.}
}

@InProceedings{lopez-paz13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Gaussian Process Vine Copulas for Multivariate Dependence},
author = {Lopez-Paz, David and Miguel Hern\'{a}ndez-Lobato, Jose and Zoubin, Ghahramani},
pages = {10-18},
abstract = {Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function.  Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables.  In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables.  We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.}
}

@InProceedings{valko13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Stochastic Simultaneous Optimistic Optimization},
author = {Valko, Michal and Carpentier, Alexandra and Munos, R\'{e}mi},
pages = {19-27},
abstract = {We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.}
}

@InProceedings{carpentier13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Toward Optimal Stratification for Stratified Monte-Carlo Integration},
author = {Carpentier, Alexandra and Munos, R\'{e}mi},
pages = {28-36},
abstract = {We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm {\em Monte-Carlo Upper-Lower Confidence Bound} that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition.}
}

@InProceedings{gong13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems},
author = {Gong, Pinghua and Zhang, Changshui and Lu, Zhaosong and Huang, Jianhua and Ye, Jieping},
pages = {37-45},
abstract = {Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.}
}

@InProceedings{tran13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Thurstonian {B}oltzmann Machines: Learning from Multiple Inequalities},
author = {Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
pages = {46-54},
abstract = {We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.}
}

@InProceedings{kim13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {A Variational Approximation for Topic Modeling of Hierarchical Corpora},
author = {Kim, Do-kyum and Voelker, Geoffrey and Saul, Lawrence},
pages = {55-63},
abstract = {We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy.  We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation.  The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs).  For these models we show that there exists a simple variational approximation for probabilistic inference.  The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model's hierarchy.  We compare our approach to existing implementations of nonparametric HDPs.  On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods.  Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security--one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.}
}

@InProceedings{goerg13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Forecastable Component Analysis},
author = {Goerg, Georg},
pages = {64-72},
abstract = {I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classication. The R package ForeCA accompanies this work and is publicly available on CRAN.}
}

@InProceedings{krummenacher13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Ellipsoidal Multiple Instance Learning},
author = {Krummenacher, Gabriel and Soon Ong, Cheng and Buhmann, Joachim},
pages = {73-81},
abstract = {We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive.  We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches.}
}

@InProceedings{lee13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Local Low-Rank Matrix Approximation},
author = {Lee, Joonseok and Kim, Seungyeon and Lebanon, Guy and Singer, Yoram},
pages = {82-90},
abstract = {Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.}
}

@InProceedings{urvoy13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Generic Exploration and {K}-armed Voting Bandits},
author = {Urvoy, Tanguy and Clerot, Fabrice and F\'{e}raud, Raphael and Naamane, Sami},
pages = {91-99},
abstract = {We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.}
}

@InProceedings{haquang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {A unifying framework for vector-valued manifold regularization and multi-view learning},
author = {H\`{a} Quang, Minh and Bazzani, Loris and Murino, Vittorio},
pages = {100-108},
abstract = {This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency  between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches.  In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning.  }
}

@InProceedings{ganeshapillai13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Learning Connections in Financial Time Series},
author = {Ganeshapillai, Gartheeban and Guttag, John and Lo, Andrew},
pages = {109-117},
abstract = {To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only ``beat the market,''  but also outperform optimal (i.e., minimum variance) portfolios.}
}

@InProceedings{wang13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Fast dropout training},
author = {Wang, Sida and Manning, Christopher},
pages = {118-126},
abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective.  This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability.  We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations. }
}

@InProceedings{yang13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Scalable Optimization of Neighbor Embedding for Visualization},
author = {Yang, Zhirong and Peltonen, Jaakko and Kaski, Samuel},
pages = {127-135},
abstract = {Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n^2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into ``forces'' exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their ``center of mass'', rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings ``big data'' within reach of visualization.}
}

@InProceedings{hanczar13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Precision-recall space to correct external indices for biclustering},
author = {Hanczar, Blaise and Nadif, Mohamed},
pages = {136-144},
abstract = {Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach. }
}

@InProceedings{wulff13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Monochromatic Bi-Clustering},
author = {Wulff, Sharon and Urner, Ruth and Ben-David, Shai},
pages = {145-153},
abstract = {We propose a natural cost function for the bi-clustering task, the monochromatic cost.  This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees  and formal NP-hardness results  for any bi-clustering optimization problem.  In addition, we show that our optimization problem can be efficiently  solved by deterministic annealing,  yielding a promising heuristic for large problem instances.}
}

@InProceedings{alain13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Gated Autoencoders with Tied Input Weights},
author = {Alain, Droniou and Olivier, Sigaud},
pages = {154-162},
abstract = {The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.}
}

@InProceedings{rebagliati13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Strict Monotonicity of Sum of Squares Error  and Normalized Cut in the Lattice of Clusterings},
author = {Rebagliati, Nicola},
pages = {163-171},
abstract = {Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings.}
}

@InProceedings{han13a,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Transition Matrix Estimation in High Dimensional Time Series},
author = {Han, Fang and Liu, Han},
pages = {172-180},
abstract = {In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging.}
}

@InProceedings{weston13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Label Partitioning For Sublinear Ranking},
author = {Weston, Jason and Makadia, Ameesh and Yee, Hector},
pages = {181-189},
abstract = {We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment  to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance.}
}

@InProceedings{wang13b,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing},
author = {Wang, Huayan and Daphne, Koller},
pages = {190-198},
abstract = {Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing messages over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a unified message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).}
}

@InProceedings{bardenet13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Collaborative hyperparameter tuning},
author = {Bardenet, R\'{e}mi and Brendel, M\'{a}ty\'{a}s and K\'{e}gl, Bal\'{a}zs and Sebag, Mich\`{e}le},
pages = {199-207},
abstract = {Hyperparameter learning has traditionally been a manual task because of the limited number of trials. Today's computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to hyperparameter learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the hyperparameter space, and they could significantly improve on manual hyperparameter tuning. What may make experienced practitioners even better at hyperparameter optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization.}
}

@InProceedings{cai13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {SADA: A General Framework to Support Robust Causation Discovery},
author = {Cai, Ruichu and Zhang, Zhenjie and Hao, Zhifeng},
pages = {208-216},
abstract = {Causality discovery without manipulation is considered a crucial problem to a variety of applications, such as genetic therapy. The state-of-the-art solutions, e.g. LiNGAM, return accurate results when the number of labeled samples is larger than the number of variables. These approaches are thus applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge strategy, named SADA, to enhance the scalability of a wide class of causality discovery algorithms. SADA is able to accurately identify the causal variables, even when the sample size is significantly smaller than the number of variables. In SADA, the variables are partitioned into subsets, by finding cuts on the sparse probabilistic graphical model over the variables. By running mainstream causation discovery algorithms, e.g. LiNGAM, on the subproblems, complete causality can be reconstructed by combining all the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scale of problems without significant sacrifice on result accuracy, depending only on the local sparsity condition over the variables. Experiments on real-world datasets verify the improvements on scalability and accuracy by applying SADA on top of existing causation algorithms.}
}

@InProceedings{sohn13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Learning and Selecting Features Jointly with Point-wise Gated {B}oltzmann Machines},
author = {Sohn, Kihyuk and Zhou, Guanyu and Lee, Chansoo and Lee, Honglak},
pages = {217-225},
abstract = {Unsupervised feature learning has emerged as a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i.e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines feature learning and feature selection. Our model performs not only feature selection on learned high-level features (i.e., hidden units), but also dynamic feature selection on raw features (i.e., visible units) through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the visible units corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks. }
}

@InProceedings{wen13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Sequential {B}ayesian Search},
author = {Wen, Zheng and Kveton, Branislav and Eriksson, Brian and Bhamidipati, Sandilya},
pages = {226-234},
abstract = {Millions of people search daily for movies, music, and books on the Internet. Unfortunately, non-personalized exploration of items can result in an infeasible number of costly interaction steps. We study the problem of efficient, repeated interactive search. In this problem, the user is navigated to the items of interest through a series of options and our objective is to learn a better search policy from past interactions with the user. We propose an efficient learning algorithm for solving the problem, sequential Bayesian search (SBS), and prove that it is Bayesian optimal. We also analyze the algorithm from the frequentist point of view and show that its regret is sublinear in the number of searches. Finally, we evaluate our method on a real-world movie discovery problem and show that it performs nearly optimally as the number of searches increases.}
}

@InProceedings{kyrillidis13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Sparse projections onto the simplex},
author = {Kyrillidis, Anastasios and Becker, Stephen and Cevher, Volkan and Koch, Christoph},
pages = {235-243},
abstract = {Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the $\ell_1$-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints.}
}

@InProceedings{shalit13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Modeling Musical Influence with Topic Models},
author = {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},
pages = {244-252},
abstract = {The role of musical influence has long been debated by scholars  and critics in the humanities, but never in a data-driven way.  In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970's and the mid 1990's.}
}

@InProceedings{das13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically},
author = {Das, Mrinal and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Kanchi, Gopinath},
pages = {253-261},
abstract = {In a recent pioneering approach LDA was used to discover cross cutting concerns(CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models(STM). STM uses a generalized stick breaking process(GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.}
}

@InProceedings{salazar13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Exploring the Mind: Integrating Questionnaires and fMRI},
author = {Salazar, Esther and Bogdan, Ryan and Gorka, Adam and Hariri, Ahmad and Carin, Lawrence},
pages = {262-270},
abstract = {A new model is developed for joint analysis of ordered, categorical, real and count data. The ordered and categorical data are answers to questionnaires, the (word) count data correspond to the text questions from the questionnaires, and the real data correspond to fMRI responses for each subject. The Bayesian model employs the von Mises distribution in a novel manner to infer sparse graphical models jointly across people, questions, fMRI stimuli and brain region, with this integrated within a new matrix factorization based on latent binary features. The model is compared with simpler alternatives on two real datasets. We also demonstrate the ability to predict the response of the brain to visual stimuli (as measured by fMRI), based on knowledge of how the associated person answered classical questionnaires.}
}

@InProceedings{trandinh13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {A proximal {N}ewton framework for composite minimization: Graph learning without {C}holesky decompositions and matrix inversions},
author = {Tran Dinh, Quoc and Kyrillidis, Anastasios and Cevher, Volkan},
pages = {271-279},
abstract = {We propose an algorithmic framework for convex minimization problems of composite functions with two terms:  a self-concordant part and a possibly nonsmooth regularization part.  Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic step-size selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations. }
}

@InProceedings{arora13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {A Practical Algorithm for Topic Modeling with Provable Guarantees},
author = {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
pages = {280-288},
abstract = {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.}
}

@InProceedings{gopal13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Distributed training of Large-scale Logistic models},
author = {Gopal, Siddharth and Yang, Yiming},
pages = {289-297},
abstract = {Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.}
}

@InProceedings{ranganath13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {An Adaptive Learning Rate for Stochastic Variational Inference},
author = {Ranganath, Rajesh and Wang, Chong and David, Blei and Xing, Eric},
pages = {298-306},
abstract = {  Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets.  It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient.  Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic inference.  Our method requires no tuning and is easily implemented with computations already made in the algorithm.  We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora.  Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates.}
}

@InProceedings{telgarsky13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Margins, Shrinkage, and Boosting},
author = {Telgarsky, Matus},
pages = {307-315},
abstract = {This manuscript shows that AdaBoost and its immediate variants can produce approximately maximum margin classifiers simply by scaling their step size choices by a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman's empirically successful ``shrinkage'' procedure for gradient boosting (Friedman, 2000).  Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.  }
}

@InProceedings{chang13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment},
author = {Chang, Billy and Kruger, Uwe and Kustra, Rafal and Zhang, Junping},
pages = {316-324},
abstract = {Canonical correlation analysis (CCA) is a well established technique for identifying linear relationships among two variable sets.  Kernel CCA (KCCA) is the most notable nonlinear extension but it lacks interpretability and robustness against irrelevant features.  The aim of this article is to introduce two nonlinear CCA extensions that rely on the recently proposed Hilbert-Schmidt independence criterion and the centered kernel target alignment.  These extensions determine linear projections that provide maximally dependent projected data pairs.  The paper demonstrates that the use of linear projections allows removing irrelevant features, whilst extracting combinations of strongly associated features.  This is exemplified through a simulation and the analysis of recorded data that are available in the literature.}
}

@InProceedings{golovin13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Large-Scale Learning with Less RAM via Randomization},
author = {Golovin, Daniel and Sculley, D. and McMahan, Brendan and Young, Michael},
pages = {325-333},
abstract = {We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50\% during training and by up 95\% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.}
}

@InProceedings{ermon13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization},
author = {Ermon, Stefano and Gomes, Carla and Sabharwal, Ashish and Selman, Bart},
pages = {334-342},
abstract = {Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.}
}

@InProceedings{maurer13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Sparse coding for multitask and transfer learning},
author = {Maurer, Andreas and Pontil, Massi and Romera-Paredes, Bernardino},
pages = {343-351},
abstract = {We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.}
}

@InProceedings{yuhui13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Direct Modeling of Complex Invariances for Visual Object Features},
author = {Yu Hui, Ka},
pages = {352-360},
abstract = {View-invariant object representations created from feature pooling networks have been widely adopted in state-of-the-art visual recognition systems. Recently, the research community seeks to improve these view-invariant representations further by additional invariance and receptive field learning, or by taking on the challenge of processing massive amounts of learning data. In this paper we consider an alternate strategy of directly modeling complex invariances of object features. While this may sound like a naive and inferior approach, our experiments show that this approach can achieve competitive and state-of-the-art accuracy on visual recognition data sets such as CIFAR-10 and STL-10. We present an highly applicable dictionary learning algorithm on complex invariances that can be used in most feature pooling network settings. It also has the merits of simplicity and requires no additional tuning. We also discuss the implication of our experiment results concerning recent observations on the usefulness of pre-trained features, and the role of direct invariance modeling in invariance learning.}
}

@InProceedings{willemvandemeent13,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Hierarchically-coupled hidden {M}arkov models for learning kinetic rates from single-molecule data},
author = {van de Meent, Jan-Willem and Bronson, Jonathan and Wood, Frank and Gonzalez Jr., Ruben and Wiggins, Chris},
pages = {361-369},
abstract = {We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts.  In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals.  Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.}
}

@InProceedings{yang13c,
booktitle = {Proceedings of The 30th International Conference on Machine Learning},
editor = {Sanjoy Dasgupta and David McAllester},
publisher   = {JMLR Workshop and Conference Proceedings},
volume = {28},
number = {2},
section = {cycle-2},
month = {May},
year = {2013},
title = {Activized Learning with Uniform Classification Noise},
author = {Yang, Liu and Hanneke, Steve},
pages = {370-378},
abstract = {We prove that for any VC class, it is possible to transform any passive learning algorithm into an active learning algorithm with strong asymptotic improvements in label complexity  for every nontrivial distribution satisfying a uniform classification noise condition.  This generalizes a similar result proven by  (Hanneke, 2009;2012) for the realizable case,  and is the first result establishing that such general improvement guarantees are possible  in the presence of restricted types of  classification noise.}
}

