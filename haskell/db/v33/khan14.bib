@InProceedings{khan14,
  supplementary = {Supplementary:khan14-supp.pdf},
  title = {{Scalable Collaborative Bayesian Preference Learning}},
  author = {Khan, Mohammad Emtiyaz and Ko, Young Jun and Seeger, Matthias},
  pages = {475-483},
  abstract = {Learning about users' utilities from preference, discrete choice or implicit feedback data is of integral importance in e-commerce, targeted advertising and web search. Due to the sparsity and diffuse nature of data, Bayesian approaches  hold much promise, yet most prior work does not scale up to realistic data sizes. We shed light on why inference for such settings is computationally   difficult for standard machine learning methods, most of which focus on predicting explicit ratings only. To simplify the difficulty, we present a novel expectation maximization algorithm, driven by expectation propagation approximate inference, which scales to very large datasets without requiring strong factorization assumptions. Our  utility model uses both latent bilinear collaborative filtering and non-parametric Gaussian process (GP) regression. In experiments on large real-world  datasets, our method gives substantially better results than either matrix factorization or GPs in isolation, and converges significantly faster.},
}
