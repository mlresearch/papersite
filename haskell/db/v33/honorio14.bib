@InProceedings{honorio14,
  supplementary = {Supplementary:honorio14-supp.pdf},
  title = {{Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees}},
  author = {Honorio, Jean and Jaakkola, Tommi},
  pages = {384-392},
  abstract = {We analyze the expected risk of linear classifiers for a fixed weight vector in the ``minimax'' setting. That is, we analyze the worst-case risk among all data distributions with a given mean and covariance. We provide a simpler proof of the tight polynomial-tail bound for general random variables. For sub-Gaussian random variables, we derive a novel tight exponential-tail bound. We also provide new PAC-Bayes finite-sample guarantees when training data is available. Our ``minimax'' generalization bounds are dimensionality-independent and $\mathcal{O}(\sqrt{1/m})$ for $m$ samples.},
}
