@InProceedings{jenatton16,
  title = {Adaptive Algorithms for Online Convex Optimization with Long-term Constraints},
  author = {Rodolphe Jenatton and Jim Huang and Cedric Archambeau},
  pages = {402-411},
  abstract = {We present an adaptive online gradient descent algorithm to solve online convex optimization problems with long-term constraints, which are constraints that need to be satisfied when accumulated over a finite number of rounds T, but can be violated in intermediate rounds. For some user-defined trade-off parameter $\beta in (0, 1)$, the proposed algorithm achieves cumulative regret bounds of $O(T^max{\beta,1_\beta})$ and $O(T^{1_\beta/2})$, respectively for the loss and the constraint violations. Our results hold for convex losses, can handle arbitrary convex constraints and rely on a single computationally efficient algorithm. Our contributions improve over the best known cumulative regret bounds of Mahdavi et al. (2012), which are respectively $O(T^{1/2})$ and $O(T^{3/4})$ for general convex domains, and respectively $O(T^{2/3})$ and $O(T^{2/3})$ when the domain is further restricted to be a polyhedral set. We supplement the analysis with experiments validating the performance of our algorithm in practice.},
}
