@InProceedings{qua16,
  supplementary = {Supplementary:qua16-supp.pdf},
  title = {Fast Rate Analysis of Some Stochastic Optimization Algorithms},
  author = {Chao Qu and Huan Xu and Chong jin Ong},
  pages = {662-670},
  abstract = {In this paper, we revisit three fundamental and popular stochastic optimization algorithms (namely, Online Proximal Gradient, Regularized Dual Averaging method and ADMM with online proximal gradient) and analyze their convergence speed under conditions weaker than those in literature. In particular, previous works showed that these algorithms converge at a rate of $O (\ln T/T)$ when the loss function is strongly convex, and $O (1 /\sqrt{T})$ in the weakly convex case. In contrast, we relax the strong convexity assumption of the loss function, and show that the algorithms converge at a rate $O (\ln T/T)$ if the {\em expectation} of the loss function is {\em locally} strongly convex. This is a much weaker assumption and is satisfied by many practical formulations including Lasso and Logistic Regression. Our analysis thus extends the applicability of these three methods, as well as provides a general recipe for improving analysis of convergence rate for stochastic and online optimization algorithms.},
}
