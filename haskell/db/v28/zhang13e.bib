@InProceedings{zhang13e,
  pdf = {http://jmlr.org/proceedings/papers/v28/zhang13e.pdf},
  supplementary = {Supplementary:zhang13e-supp.pdf},
  number = {3},
  section = {cycle-3},
  title = {O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions},
  author = {Zhang, Lijun and Yang, Tianbao and Jin, Rong and He, Xiaofei},
  pages = {1121-1129},
  abstract = {Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result.},
}
