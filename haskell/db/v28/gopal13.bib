@InProceedings{gopal13,
  pdf = {http://jmlr.org/proceedings/papers/v28/gopal13.pdf},
  number = {2},
  section = {cycle-2},
  title = {Distributed training of Large-scale Logistic models},
  author = {Gopal, Siddharth and Yang, Yiming},
  pages = {289-297},
  abstract = {Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.},
}
