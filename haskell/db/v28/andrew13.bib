@InProceedings{andrew13,
  pdf = {http://jmlr.org/proceedings/papers/v28/andrew13.pdf},
  number = {3},
  section = {cycle-3},
  title = {Deep Canonical Correlation Analysis},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  pages = {1247-1255},
  abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emph{canonical correlation analysis} (CCA).  It is an alternative to the nonparametric method \emph{kernel canonical correlation analysis} (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.},
}
