@InProceedings{gupta13a,
  pdf = {http://jmlr.org/proceedings/papers/v28/gupta13a.pdf},
  number = {3},
  section = {cycle-3},
  title = {Factorial Multi-Task Learning : A Bayesian Nonparametric Approach},
  author = {Gupta, Sunil and Phung, Dinh and Venkatesh, Svetha},
  pages = {657-665},
  abstract = {Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods.},
}
