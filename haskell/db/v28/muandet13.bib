@InProceedings{muandet13,
  pdf = {http://jmlr.org/proceedings/papers/v28/muandet13.pdf},
  supplementary = {Supplementary:muandet13-supp.pdf},
  title = {Domain Generalization via Invariant Feature Representation},
  number = {1},
  section = {cycle-1},
  author = {Muandet, Krikamol and Balduzzi, David and Sch\"{o}lkopf, Bernhard},
  pages = {10-18},
  abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.  },
}
