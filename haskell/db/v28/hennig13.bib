@InProceedings{hennig13,
  pdf = {http://jmlr.org/proceedings/papers/v28/hennig13.pdf},
  title = {Fast Probabilistic Optimization from Noisy Gradients},
  author = {Hennig, Philipp},
  number = {1},
  section = {cycle-1},
  pages = {62-70},
  abstract = {Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.},
}
