@InProceedings{zhang13d,
  pdf = {http://jmlr.org/proceedings/papers/v28/zhang13d.pdf},
  supplementary = {Supplementary:zhang13d-supp.pdf},
  number = {3},
  section = {cycle-3},
  title = {Domain Adaptation under Target and Conditional Shift},
  author = {Zhang, Kun and Schlkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
  pages = {819-827},
  abstract = {Let $X$ denote the feature and $Y$ the target. We consider domain adaptation under three possible scenarios: (1) the marginal $P_Y$ changes, while the conditional $P_{X|Y}$ stays the same ({\it target shift}), (2) the marginal $P_Y$ is fixed, while the conditional $P_{X|Y}$ changes with certain constraints ({\it conditional shift}), and (3) the marginal $P_{Y}$ changes, and the conditional $P_{X|Y}$ changes with constraints ({\it generalized target shift}). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by {\it reweighting or transforming training data to reproduce the covariate distribution} on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.},
}
