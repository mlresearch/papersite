@InProceedings{mehta13,
  pdf = {http://jmlr.org/proceedings/papers/v28/mehta13.pdf},
  supplementary = {Supplementary:mehta13-supp.pdf},
  title = {Sparsity-Based Generalization Bounds for Predictive Sparse Coding},
  number = {1},
  section = {cycle-1},
  author = {Mehta, Nishant and Gray, Alexander},
  pages = {36-44},
  abstract = {The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations.},
}
