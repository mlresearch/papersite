@InProceedings{wan13,
  pdf = {http://jmlr.org/proceedings/papers/v28/wan13.pdf},
  supplementary = {Supplementary:wan13-supp.pdf},
  number = {3},
  section = {cycle-3},
  title = {Regularization of Neural Networks using DropConnect},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  pages = {1058-1066},
  abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
}
