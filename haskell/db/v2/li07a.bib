@InProceedings{li07a,
  title = {A Bayesian Divergence Prior for Classiffier Adaptation},
  author = {Xiao Li and Jeff Bilmes},
  pages = {275--282},
  abstract = {Adaptation of statistical classifiers is critical when a target (or testing) distribution is different from the distribution that governs training data. In such cases, a classifier optimized for the training distribution needs to be adapted for optimal use in the target distribution. This paper presents a Bayesian ``divergence prior'' for generic classifier adaptation. Instantiations of this prior lead to simple yet principled adaptation strategies for a variety of classifiers, which yield superior performance in practice. In addition, this paper derives several adaptation error bounds by applying the divergence prior in the PAC-Bayesian setting.},
  pdf = {http://jmlr.org/proceedings/papers/v2/li07a/li07a.pdf},
}
