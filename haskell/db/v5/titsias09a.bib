@InProceedings{titsias09a,
  title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  author = {Michalis Titsias},
  pages = {567--574},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  pdf = {http://jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf},
}
