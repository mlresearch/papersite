@InProceedings{perolat15,
  supplementary = {Supplementary:perolat15-supp.pdf},
  title = {Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games},
  author = {Julien Perolat and Bruno Scherrer and Bilal Piot and Olivier Pietquin},
  pages = {1321-1329},
  abstract = {This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in $L_p$-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is $\frac{2\gamma}{(1 - \gamma)^2} \epsilon + \frac{1}{(1 - \gamma)^2}\epsilon'$-optimal, where $\epsilon$ is the value function approximation error and $\epsilon'$ is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-$Q$) to solve infinite horizon $\gamma$-discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted-$Q$ algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-$Q$ on a simultaneous two-player game, namely Alesia.},
}
