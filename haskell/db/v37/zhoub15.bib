@InProceedings{zhoub15,
  supplementary = {Supplementary:zhoub15-supp.pdf},
  title = {$\ell_{1,p}$-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods},
  author = {Zirui Zhou and Qi Zhang and Anthony Man-Cho So},
  pages = {1501-1510},
  abstract = {Recently, $\ell_{1,p}$-regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of $\ell_{1,p}$-regularized problems, an error bound condition is satisfied when $p\in[1,2]$ or $p=\infty$ but fails to hold for any $p\in(2,\infty)$. Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to $\ell_{1,p}$-regularized linear or logistic regression with $p\in[1,2]$ or $p=\infty$. By contrast, numerical experiments suggest that for the same class of problems with $p\in(2,\infty)$, the aforementioned methods may not converge linearly.},
}
