@InProceedings{gonga15,
  supplementary = {Supplementary:gonga15-supp.pdf},
  title = {A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis},
  author = {Pinghua Gong and Jieping Ye},
  pages = {276-284},
  abstract = {The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the $\ell_1$-regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to $\ell_1$-regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any $\ell_1$-regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving $\ell_1$-regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN.},
}
