@InProceedings{pan13a,
  pdf = {http://jmlr.org/proceedings/papers/v31/pan13a.pdf},
  supplementary = {Supplementary:pan13a-supp.pdf},
  title = {High-dimensional Inference via Lipschitz Sparsity-Yielding Regularizers},
  author = {Pan, Zheng and Zhang, Changshui},
  pages = {481--488},
  abstract = {Non-convex regularizers are more and more applied to high-dimensional inference with sparsity prior knowledge. In general, the non-convex regularizer is superior to the convex ones in inference but it suffers the difficulties brought by local optimums and massive computation. A "good" regularizer should perform well in both inference and optimization. In this paper, we prove that some non-convex regularizers can be such "good" regularizers. They are a family of sparsity-yielding penalties with proper Lipschitz subgradients. These regularizers keep the superiority of non-convex regularizers in inference. Their estimation conditions based on sparse eigenvalues are weaker than the convex regularizers. Meanwhile, if properly tuned, they behave like convex regularizers since standard proximal methods guarantee to give stationary solutions. These stationary solutions, if sparse enough, are identical to the global solutions. If the solution sequence provided by proximal methods is along a sparse path, the convergence rate to the global optimum is on the order of $1/k$ where $k$ is the number of iterations.},
}
