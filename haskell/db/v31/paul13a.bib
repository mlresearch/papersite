@InProceedings{paul13a,
  pdf = {http://jmlr.org/proceedings/papers/v31/paul13a.pdf},
  title = {Random Projections for Support Vector Machines},
  author = {Paul, Saurabh and Boutsidis, Christos and Magdon-Ismail, Malik and Drineas, Petros},
  pages = {498--506},
  abstract = {Let $X$ be a data matrix of rank $\rho$, representing $n$ points in $d$-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix $X$. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within $\epsilon$-relative error, ensuring comparable generalization as in the original space. We present extensive experiments with real and synthetic data to support our theory.},
}
