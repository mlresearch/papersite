@InProceedings{smetsers14a,
  author = {Smetsers, Rick and   Volpato, Michele and   Vaandrager, Frits and  Verwer, Sicco},
  title = {Bigger is Not Always Better: on the Quality of Hypotheses in Active Automata Learning},
  pages = {167-181},
  abstract = {In Angluin's $L^{\ast}$ algorithm a learner constructs a sequence of hypotheses
in order to learn a regular language. Each hypothesis is consistent with a
larger set of observations and is described by a bigger model. From a
behavioral perspective, however, a hypothesis is not always better than the
previous one, in the sense that the minimal length of a counterexample that
distinguishes a hypothesis from the target language may decrease.  We present a
simple modification of the $L^{\ast}$ algorithm that ensures that for
subsequent hypotheses the minimal length of a counterexample never
decreases, which implies that the distance to the target language never
increases in a corresponding ultrametric.  Preliminary experimental evidence
suggests that our algorithm speeds up learning in practical applications by
reducing the number of equivalence queries.},
}
