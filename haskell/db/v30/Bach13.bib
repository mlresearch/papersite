@InProceedings{Bach13,
  pdf = {http://jmlr.org/proceedings/papers/v30/Bach13.pdf},
  title = {Sharp analysis of low-rank  kernel matrix approximations},
  author = {Francis Bach},
  pages = {185-209},
  abstract = {We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With  kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations $n$, i.e., $O(n^2)$. Low-rank approximations of the kernel matrix are often considered as they allow the reduction of  running time complexities  to $O(p^2 n)$, where~$p$ is the rank of the approximation. The practicality of such methods thus depends on the required rank $p$. In this paper, we show that for approximations based on a random subset of columns of the original kernel matrix, the rank $p$ may be chosen to be linear in the \emph{degrees of freedom} associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same \emph{predictive performance} than existing algorithms, for any given problem instance, and not only for worst-case situations.},
}
