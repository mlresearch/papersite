@InProceedings{lo09,
  title = {An Ensemble of Three Classifiers for KDD Cup 2009: Expanded Linear Model, Heterogeneous Boosting, and Selective Naive Bayes},
  author = {Hung-Yi Lo and Kai-Wei Chang and Shang-Tse Chen and Tsung-Hsien Chiang and Chun- Sung Ferng and Cho-Jui Hsieh and Yi-Kuang Ko and Tsung-Ting Kuo and Hung-Che Lai and Ken-Yi Lin and Chia-Hsuan Wang and Hsiang-Fu Yu and Chih-Jen Lin and Hsuan-Tien Lin and Shou-de Lin},
  pages = {57--64},
  abstract = {This paper describes our ensemble of three classifiers for the KDD Cup 2009 challenge. First, we transform the three binary classification tasks into a joint multi-class classification problem, and solve an l1-regularized maximum entropy model under the LIBLINEAR framework. Second, we propose a heterogeneous base learner, which is capable of handling different types of features and missing values, and use AdaBoost to improve the base learner. Finally, we adopt a selective na√Øve Bayes classifier that automatically groups categorical features and discretizes numerical ones. The parameters are tuned using crossvalidation results rather than the 10\% test results on the competition website. Based on the observation that the three positive labels are exclusive, we conduct a post-processing step using the linear SVM to jointly adjust the prediction scores of each classifier on the three tasks. Then, we average these prediction scores with careful validation to get the final outputs. Our final average AUC on the whole test set is 0.8461, which ranks third place in the slow track of KDD Cup 2009.},
  pdf = {http://jmlr.org/proceedings/papers/v7/lo09/lo09.pdf},
}
