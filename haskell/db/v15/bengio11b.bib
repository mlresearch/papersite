@InProceedings{bengio11b,
  title = {Deep Learners Benefit More from Out-of-Distribution Examples},
  author = {Yoshua Bengio and FrÃ©dÃ©ric Bastien and Arnaud Bergeron and Nicolas Boulanger–Lewandowski and Thomas Breuel and Youssouf Chherawala and Moustapha Cisse and Myriam CÃ´tÃ© and Dumitru Erhan and Jeremy Eustache and Xavier Glorot and Xavier Muller and Sylvain Pannetier Lebeuf and Razvan Pascanu and Salah Rifai and FranÃ§ois Savard and Guillaume Sicard},
  pages = {164--172},
  abstract = {Recent theoretical and empirical work in statistical machine learning has demonstrated the potential of learning algorithms for deep architectures, i.e., function classes obtained by composing multiple levels of representation. The hypothesis evaluated here is that intermediate levels of representation, because they can be shared across tasks and examples from different but related distributions, can yield even more benefits. Comparative experiments were performed on a large-scale handwritten character recognition setting with 62 classes (upper case, lower case, digits), using both a multi-task setting and perturbed examples in order to obtain out-of-distribution examples. The results agree with the hypothesis, and show that a deep learner did {\em beat previously published results and reached human-level performance}.

[pdf]},
  pdf = {http://jmlr.org/proceedings/papers/v15/bengio11b/bengio11b.pdf},
}
