@InProceedings{lin14,
  pdf = {http://jmlr.org/proceedings/papers/v32/lin14.pdf},
  section = {cycle-1},
  supplementary = {Supplementary:lin14-supp.pdf},
  title = {An Adaptive Accelerated Proximal Gradient Method and its Homotopy Continuation for Sparse Optimization},
  author = {Qihang Lin and Lin Xiao},
  pages = {73-81},
  abstract = {We first propose an adaptive accelerated proximal gradient(APG) method for minimizing strongly convex composite functions with unknown convexity parameters. This method incorporates a restarting scheme to automatically estimate the strong convexity parameter and achieves a nearly optimal iteration complexity. Then we consider the ℓ1-regularized least-squares (ℓ1-LS) problem in the high-dimensional setting. Although such an objective function is not strongly convex, it has restricted strong convexity over sparse vectors. We exploit this property by combining the adaptive  APG method with a homotopy continuation scheme, which generates a sparse solution path towards optimality. This method obtains a global linear rate of convergence and its overall iteration complexity has a weaker dependency on the restricted condition number than previous work.},
}
