@InProceedings{shamir14,
  supplementary = {Supplementary:shamir14-supp.pdf},
  title = {Communication-Efficient Distributed Optimization using an Approximate Newton-type Method},
  author = {Ohad Shamir and Nati Srebro and Tong Zhang},
  pages = {1000-1008},
  abstract = {We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \emph{improves} with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.},
  section = {cycle-2},
}
