@InProceedings{zhong14,
  pdf = {http://jmlr.org/proceedings/papers/v32/zhong14.pdf},
  section = {cycle-1},
  title = {Fast Stochastic Alternating Direction Method of Multipliers},
  author = {Wenliang Zhong and James Kwok},
  pages = {46-54},
  abstract = {We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms,  it improves the convergence rate on convex problems from $\mO(1/\sqrt{T})$ to $\mO(1/T)$, where $T$ is the number of iterations. This matches the  convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.},
}
