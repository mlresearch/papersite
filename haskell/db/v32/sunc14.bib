@InProceedings{sunc14,
  supplementary = {Supplementary:sunc14-supp.pdf},
  title = {A Convergence Rate Analysis for LogitBoost, MART and Their Variant},
  author = {Peng Sun and Tong Zhang and Jie Zhou},
  pages = {1251-1259},
  abstract = {LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has $O(\frac{1}{T})$ rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.},
  section = {cycle-2},
}
