@InProceedings{andoni14,
  title = {Learning Polynomials with Neural Networks},
  author = {Alexandr Andoni and Rina Panigrahy and Gregory Valiant and Li Zhang},
  pages = {1908-1916},
  abstract = {We study the effectiveness of learning low degree polynomials using   neural networks by the gradient descent method.  While neural   networks have been shown to have great expressive power, and gradient   descent has been widely used in practice for learning neural   networks, few theoretical guarantees are known for such methods.  In   particular, it is well known that gradient descent can get stuck at   local minima, even for simple classes of target functions.  In this   paper, we present several positive theoretical results to support the   effectiveness of neural networks.  We focus on two-layer neural   networks (i.e. one hidden layer) where the top layer node is a linear   function, similar to~\cite{barron93}.  First we show that for a   randomly initialized neural network with sufficiently many hidden   units, the gradient descent method can learn any low degree   polynomial.  Secondly, we show that if we use complex-valued weights   (the target function can still be real), then under suitable   conditions, there are no ``robust local minima'': the neural network   can always escape a local minimum by performing a random   perturbation. This property does not hold for real-valued weights.   Thirdly, we discuss whether sparse polynomials can be learned   with \emph{small} neural networks, where the size is dependent on the   sparsity of the target function.},
  section = {cycle-2},
}
