@InProceedings{lee15a,
  title = {{Deeply-Supervised Nets}},
  author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  pages = {562-570},
  abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures:  (1) transparency in the effect intermediate layers have on overall classification;  (2) discriminativeness and robustness of learned features, especially in early layers;  (3) training effectiveness in the face of ``vanishing'' gradients.  To combat these issues, we introduce ``companion'' objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN.},
}
