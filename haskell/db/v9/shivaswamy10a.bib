@InProceedings{shivaswamy10a,
  title = {Empirical Bernstein Boosting},
  author = {Pannagadatta Shivaswamy and Tony Jebara},
  pages = {733--740},
  abstract = {Concentration inequalities that incorporate variance information (such as Bernstein's or Bennett's inequality) are often significantly tighter than counterparts (such as Hoeffding's inequality) that disregard variance. Nevertheless, many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding's inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle--sample variance penalization--which is motivated from an empirical version of Bernstein's inequality.  This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization.},
  pdf = {http://jmlr.org/proceedings/papers/v9/shivaswamy10a/shivaswamy10a.pdf},
}
